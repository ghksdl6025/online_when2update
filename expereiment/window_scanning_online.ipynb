{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9849d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from river import stream,tree,metrics\n",
    "import utils\n",
    "from encoding import prefix_bin\n",
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import os,json\n",
    "import datetime\n",
    "from collections import deque\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a169f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measurement(metrics, type='Accuracy'):\n",
    "    if type == 'Accuracy':\n",
    "        return_value = metrics.Accuracy()\n",
    "    elif type =='F1':\n",
    "        return_value = metrics.F1()\n",
    "    elif type =='ROCAUC':\n",
    "        return_value = metrics.ROCAUC()\n",
    "    elif type == 'WeightedF1':\n",
    "        return_value = metrics.WeightedF1()\n",
    "        \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2803192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(row_counting, total_length, interval=2000):\n",
    "    if rowcounter%interval == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running cases: %s'%(len(streaming_db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8620d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_cases(new_case, test_cases):\n",
    "    test_cases.append(new_case)\n",
    "    if len(test_cases) > test_size:\n",
    "        test_cases.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961bb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Start Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 15}\n",
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19240/2696515190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    269\u001b[0m                     \u001b[0mlabel_dist_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m     \u001b[0mdist_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_dist_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m     \u001b[0mdist_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mdist_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Case count'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Experiment settings\n",
    "'./DATA/logs/synthetic_log_bc1.csv'\n",
    "dataset_label = 'bpic17'\n",
    "monitoring = 'updating'\n",
    "\n",
    "\n",
    "# Invoke parameters for dataset\n",
    "window_size = 100\n",
    "test_size = 30\n",
    "with open('../dataset_parameters.json','r') as json_file:\n",
    "    parameters = json.load(json_file)[dataset_label]\n",
    "    print(parameters)\n",
    "    key_pair = parameters['key_pair']\n",
    "    catatars = parameters['categorical_attrs']\n",
    "    maximum_prefixs = parameters['maximum_prefix']\n",
    "\n",
    "maximum_prefixs =2\n",
    "for maximum_prefix in range(1, maximum_prefixs+1):\n",
    "\n",
    "    training_windows = []\n",
    "\n",
    "    print(maximum_prefix)\n",
    "    training_window_dict = dict()\n",
    "    test_window_dict = dict()\n",
    "    \n",
    "    dataset_loc = '../DATA/logs/'+ dataset_label +'.csv'\n",
    "    try:\n",
    "        os.makedirs('../result/%s'%(dataset_label))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Set streaming classifier\n",
    "    classifier = 'efdt'\n",
    "    if classifier == 'htc':\n",
    "        streaming_classifier = tree.HoeffdingTreeClassifier(grace_period=10, split_criterion='info_gain')\n",
    "    elif classifier =='hatc':\n",
    "        streaming_classifier = tree.HoeffdingAdaptiveTreeClassifier(grace_period=10, split_criterion='info_gain' )\n",
    "    elif classifier == 'efdt':\n",
    "        streaming_classifier = tree.ExtremelyFastDecisionTreeClassifier(grace_period=10, split_criterion='info_gain')\n",
    "        \n",
    "    dataset = stream.iter_csv(\n",
    "            dataset_loc\n",
    "            )\n",
    "\n",
    "    totallength = len(list(dataset))\n",
    "\n",
    "    dataset = stream.iter_csv(\n",
    "            dataset_loc,\n",
    "            # drop=['Complete Timestamp'],\n",
    "            target='outcome'\n",
    "            )\n",
    "    enctype = 'Index-base'\n",
    "    \n",
    "    streaming_db ={}\n",
    "    training_models ={}\n",
    "    test_cases = deque()\n",
    "    feature_matrix ={}\n",
    "\n",
    "    save_model = {}\n",
    "    casecount = 0\n",
    "    rowcounter = 0\n",
    "    finished_db ={}\n",
    "    running_case = 0\n",
    "    window_acc_dict = {}\n",
    "    prediction_result = dict()\n",
    "    for i in range(1, maximum_prefix+1): prediction_result[i] = {}\n",
    "    finished_caseid = set()\n",
    "    usedingrace = set()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Before test and training streaming event predictive monitoring, grace period is preceded to initialize model by prefix length\n",
    "    and obtain feature matrix to transform future events\n",
    "    '''\n",
    "    for x,y in dataset:\n",
    "#         display_progress(rowcounter, totallength)\n",
    "        rowcounter +=1\n",
    "        # Event stream change dictionary keys\n",
    "        x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "        if dataset_label !='bpic15':\n",
    "            x['ts'] = x['ts'][:-4]\n",
    "\n",
    "        x['outcome'] =y \n",
    "        # Initialize case by prefix length        \n",
    "        caseid = x['caseid']\n",
    "        outcome = x['outcome']\n",
    "    #     progress = x['progress']\n",
    "\n",
    "        x.pop('caseid')\n",
    "        x.pop('outcome')\n",
    "\n",
    "    #     x.pop('progress')\n",
    "\n",
    "        case_bin = prefix_bin(caseid, x)\n",
    "        case_bin.set_enctype(enctype)\n",
    "\n",
    "        if caseid not in list(streaming_db.keys()):\n",
    "            case_bin.set_prefix_length(1)    \n",
    "            streaming_db[caseid] = []\n",
    "        elif caseid in finished_caseid:\n",
    "            pass\n",
    "        else:\n",
    "            case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "            case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "        # Encode event and cases and add to DB\n",
    "        case_bin.update_truelabel(outcome)   \n",
    "        case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "        ts = case_bin.event['ts']\n",
    "        streaming_db[caseid].append(case_bin)\n",
    "        # Detect label appeared case \n",
    "        if outcome != '' and caseid not in finished_caseid:\n",
    "            usedingrace.add(caseid)\n",
    "            for i in streaming_db[caseid]:\n",
    "                i.update_truelabel(outcome)\n",
    "            finished_caseid.add(caseid)\n",
    "            # Adding newly finished case to training set.    \n",
    "            casecount +=1\n",
    "            # Grace period to collect feature matrix\n",
    "            if casecount < 200:\n",
    "                case_length = len(streaming_db[caseid])\n",
    "                if case_length >= maximum_prefix:\n",
    "                    if 'prefix_%s'%(maximum_prefix) not in list(feature_matrix.keys()):\n",
    "                        feature_matrix['prefix_%s'%(maximum_prefix)]=set()\n",
    "                        training_models['prefix_%s'%(maximum_prefix)] = [streaming_classifier,\n",
    "                                                                   0]\n",
    "                    feature_list = list(streaming_db[caseid][maximum_prefix-1].encoded.keys())\n",
    "                    for x in feature_list: feature_matrix['prefix_%s'%(maximum_prefix)].add(x) \n",
    "\n",
    "            else:\n",
    "                break\n",
    "    casecount_=0\n",
    "    for caseid in list(usedingrace):\n",
    "\n",
    "        case_length = len(streaming_db[caseid])\n",
    "        if case_length >= maximum_prefix:\n",
    "            x = streaming_db[caseid][maximum_prefix-1]\n",
    "            if x.prefix_length != 0:            \n",
    "                x.encoded = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                outcome = x.true_label\n",
    "                training_models['prefix_%s'%(x.prefix_length)][0].learn_one(x.encoded,outcome)\n",
    "                training_models['prefix_%s'%(x.prefix_length)][1] +=1\n",
    "                training_windows.append(streaming_db[caseid][maximum_prefix-1])\n",
    "            update_test_cases(streaming_db[caseid], test_cases)\n",
    "\n",
    "        prediction_result[maximum_prefix][casecount_] = {}\n",
    "        y_truelist = []\n",
    "        y_predlist = []\n",
    "        for case in test_cases:\n",
    "            if len(case) >= maximum_prefix:\n",
    "                x = case[maximum_prefix-1]\n",
    "                if x.prefix_length != 0:            \n",
    "                    model = training_models['prefix_%s'%(x.prefix_length)][0]\n",
    "                    y_pred = training_models['prefix_%s'%(x.prefix_length)][0].predict_proba_one(x.encoded)\n",
    "                    y_truelist.append(x.true_label)\n",
    "                    y_predlist.append(y_pred)\n",
    "        prediction_result[maximum_prefix][casecount_]['y_true'] = y_truelist\n",
    "        prediction_result[maximum_prefix][casecount_]['y_pred'] = y_predlist\n",
    "        casecount_ +=1\n",
    "    \n",
    "    '''\n",
    "    Streaming event label prediction start.\n",
    "    - Test and training steps are executed when case finished/ event arrived with label\n",
    "    '''\n",
    "    for i in streaming_db.keys(): usedingrace.add(i)\n",
    "    streaming_db ={}\n",
    "    cdhappend ={}\n",
    "    for i in range(1, maximum_prefix+1): cdhappend[i] = 0\n",
    "    for x,y in dataset:\n",
    "#         display_progress(rowcounter, totallength)\n",
    "\n",
    "        rowcounter +=1\n",
    "        # Event stream change dictionary keys\n",
    "        x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "        if dataset_label !='bpic15':\n",
    "            x['ts'] = x['ts'][:-4]\n",
    "\n",
    "        # Check label possible\n",
    "        # x = utils.set_label(x)\n",
    "        x['outcome'] =y \n",
    "        # Initialize case by prefix length\n",
    "        caseid = x['caseid']\n",
    "        outcome = x['outcome']\n",
    "        x.pop('caseid')\n",
    "        x.pop('outcome')\n",
    "\n",
    "        if caseid not in usedingrace:\n",
    "            case_bin = prefix_bin(caseid, x)\n",
    "            case_bin.set_enctype(enctype)\n",
    "\n",
    "            if caseid not in list(streaming_db.keys()):\n",
    "                case_bin.set_prefix_length(1)    \n",
    "                streaming_db[caseid] = []\n",
    "                running_case +=1\n",
    "            elif caseid in finished_caseid:\n",
    "                pass\n",
    "            else:\n",
    "                case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "            # Encode event and cases and add to DB\n",
    "            case_bin.update_truelabel(outcome)   \n",
    "            case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "            ts = case_bin.event['ts']\n",
    "\n",
    "            if case_bin.prefix_length == maximum_prefix:\n",
    "                case_bin.encoded = utils.readjustment_training(case_bin.encoded, feature_matrix['prefix_%s'%(case_bin.prefix_length)])\n",
    "            streaming_db[caseid].append(case_bin)\n",
    "\n",
    "            # Detect label appeared case \n",
    "            if outcome != '' and caseid not in finished_caseid:\n",
    "                finished_caseid.add(caseid)\n",
    "\n",
    "                # Adding newly finished case to training set.\n",
    "                casecount +=1    \n",
    "\n",
    "                # Modify encoded attributes of cases with feature matrix\n",
    "                case_length = len(streaming_db[caseid])\n",
    "                if case_length >= maximum_prefix:\n",
    "\n",
    "                    update_test_cases(streaming_db[caseid], test_cases)\n",
    "                    streaming_db[caseid][maximum_prefix-1].update_truelabel(outcome)\n",
    "                    x = streaming_db[caseid][maximum_prefix-1].encoded\n",
    "                    prefix_length =streaming_db[caseid][maximum_prefix-1].prefix_length\n",
    "                    training_models['prefix_%s'%(prefix_length)][0].learn_one(x, outcome)\n",
    "                    training_models['prefix_%s'%(prefix_length)][1] +=1\n",
    "\n",
    "                    y_pred = training_models['prefix_%s'%(prefix_length)][0].predict_one(x)\n",
    "                    model_update_count = training_models['prefix_%s'%(prefix_length)][1]\n",
    "                    \n",
    "                    save_model['%s_%s'%(prefix_length, model_update_count)] = training_models['prefix_%s'%(streaming_db[caseid][maximum_prefix-1].prefix_length)][0]\n",
    "\n",
    "                    training_windows.append(streaming_db[caseid][maximum_prefix-1])\n",
    "                    training_window_dict[casecount] = copy.deepcopy(training_windows)\n",
    "                    test_window_dict[casecount] = copy.deepcopy([c[0] for c in test_cases if len(c) >= maximum_prefix ])\n",
    "\n",
    "                y_truelist = []\n",
    "                y_predlist = []\n",
    "\n",
    "                for case in test_cases:\n",
    "                    if len(case) >= maximum_prefix:\n",
    "                        x = case[maximum_prefix-1]\n",
    "                        if x.prefix_length != 0:\n",
    "                            length = x.prefix_length\n",
    "                            y_pred = training_models['prefix_%s'%(x.prefix_length)][0].predict_proba_one(x.encoded)\n",
    "                            y_truelist.append(x.true_label)\n",
    "                            y_predlist.append(y_pred)\n",
    "\n",
    "                prediction_result[maximum_prefix][casecount] = {}\n",
    "                prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "                prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "                if 'b1' not in caseid and cdhappend[maximum_prefix] == 0:\n",
    "                    cdhappend[maximum_prefix] = model_update_count\n",
    "\n",
    "    label_dist_dict = {}\n",
    "    label_dist_dict[0] = {'False': 0,'True':0}\n",
    "    for t in range(1, casecount+1):\n",
    "        if t not in training_window_dict.keys():\n",
    "            label_dist_dict[t] = label_dist_dict[t-1]\n",
    "        else:            \n",
    "            window = training_window_dict[t]\n",
    "            label_dist_dict[t] = {'False': 0,'True':0}\n",
    "            for i in range(len(window)):\n",
    "                if window[i].true_label == 'False':\n",
    "                    label_dist_dict[t]['False'] +=1\n",
    "                else:\n",
    "                    label_dist_dict[t]['True'] +=1\n",
    "                    \n",
    "    dist_df = pd.DataFrame.from_dict(label_dist_dict).T\n",
    "    dist_df = dist_df.reset_index(drop=False)\n",
    "    dist_df = dist_df.rename(columns={'index':'Case count'})\n",
    "    print(dist_df)\n",
    "    dist_df.to_csv('../result/bpic17/%s_prefix_%s_%s_training_window_label.csv'%(classifier, maximum_prefix, monitoring), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb66dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_window_dict[201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1591a402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Application_617110337'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_window_dict[201][3].caseid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
