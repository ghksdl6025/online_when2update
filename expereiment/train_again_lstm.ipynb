{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9fd822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from river import stream,tree,metrics\n",
    "from encoding import prefix_bin\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import os,json\n",
    "import datetime\n",
    "from collections import deque, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime, time\n",
    "import utils\n",
    "import sliding_window\n",
    "import psutil\n",
    "\n",
    "import copy\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81aad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch cuda setting\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222f0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(row_counting, total_length, interval=10000):\n",
    "    if rowcounter%interval == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running cases: %s'%(len(streaming_db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b40cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset():\n",
    "    def __init__(self, cdataset):\n",
    "        '''\n",
    "        Convert dataset to tensor\n",
    "        \n",
    "        Params\n",
    "        dataset_type: Type of dataset, trainset, validset, and testset\n",
    "        '''\n",
    "        self.cdataset = cdataset\n",
    "\n",
    "\n",
    "    def preprocessing(self):\n",
    "        self.x_data=self.cdataset[0]\n",
    "        self.y_data=self.cdataset[1]\n",
    "\n",
    "        x = self.x_data.to_numpy()\n",
    "        x = np.reshape(x, (x.shape[0],1, x.shape[1]))\n",
    "        y_set = sorted(set(self.y_data))\n",
    "        train_y =[]\n",
    "        for y in self.y_data:\n",
    "            train_y.append(y_set.index(y))\n",
    "\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float)\n",
    "        y_tensor = torch.tensor(train_y, dtype=torch.long)\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    \n",
    "    def test_preprocessing(self):\n",
    "        self.x_data=self.cdataset\n",
    "\n",
    "        x = self.x_data.to_numpy()\n",
    "        x = np.reshape(x, (x.shape[0],1, x.shape[1]))\n",
    "\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        return x_tensor\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce1a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module): # nn.Module inherit\n",
    "\n",
    "    def __init__(self, input_x, raw_y):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        \n",
    "        self.input_size = input_x.shape[2]\n",
    "        self.hidden_size =2* input_x.shape[2]\n",
    "        self.num_case = 1\n",
    "        self.num_layers =2\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers, dropout=0.25, batch_first =False, bidirectional = False)\n",
    "\n",
    "        self.h0 = torch.randn(self.num_layers, 1, self.hidden_size, device=device)\n",
    "        self.c0 = torch.randn(self.num_layers, 1, self.hidden_size, device=device)\n",
    "\n",
    "        latent_vector_size =30 * 1\n",
    "        self.linear1 = nn.Linear(1 *self.num_case *self.hidden_size, latent_vector_size)\n",
    "        self.linear_h = nn.Linear(1 *self.num_layers *self.hidden_size, latent_vector_size)\n",
    "        self.linear_o = nn.Linear(3 * latent_vector_size, 1 *self.num_case * len(set(raw_y)))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        output, (hn,cn) = self.lstm(input_x, (self.h0,self.c0))\n",
    "        output = output.reshape((output.size()[0] *output.size()[1] *output.size()[2]))\n",
    "        output = self.relu(self.linear1(output))\n",
    "\n",
    "        uH = F.leaky_relu(self.linear_h(hn.reshape((hn.size()[0] *hn.size()[1] *hn.size()[2]))))\n",
    "        uC = F.leaky_relu(self.linear_h(cn.reshape((cn.size()[0] *cn.size()[1] *cn.size()[2]))))\n",
    "        output = torch.cat((uH ,uC ,output))\n",
    "        output = self.sigmoid(self.linear_o(output))\n",
    "        output =output.reshape(self.num_case,-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc21b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_stage(datafortraining):\n",
    "    '''\n",
    "    Manage training stage of streaming anomaly detection\n",
    "    ----------\n",
    "    Parameters\n",
    "    window: class training_window\n",
    "        Sliding window with training data\n",
    "    training_models: dict\n",
    "        Trained detector by prefix stored in. Default is randomforest\n",
    "    ----------\n",
    "    Return\n",
    "    training_models\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_x,train_y = Customdataset(datafortraining).preprocessing()\n",
    "    loss_list =[]\n",
    "\n",
    "    x_tensor = torch.unsqueeze(train_x[0], dim=0)\n",
    "    y_tensor = torch.unsqueeze(train_y[0], dim=0)\n",
    "    model = LSTM_model(x_tensor, y_tensor).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "    optimizer.zero_grad()\n",
    "    loss = nn.BCELoss ()\n",
    "    losses = []\n",
    "\n",
    "    previous_model =0\n",
    "    for i in range(5):\n",
    "        running_loss =0\n",
    "        for pos, x2 in enumerate(train_x):\n",
    "            x_tensor = torch.unsqueeze(x2, dim=0)\n",
    "            y_tensor = torch.tensor([[float(train_y[pos])]])\n",
    "\n",
    "            x_tensor = x_tensor.cuda()\n",
    "            y_tensor = y_tensor.cuda()\n",
    "            output = model(x_tensor)\n",
    "#             running_loss += loss.item()\n",
    "            l = loss(output, y_tensor)\n",
    "            running_loss +=l.item()\n",
    "\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        loss_list.append(running_loss)\n",
    "        previous_model = model\n",
    "        epoch_loss = running_loss / len(train_x)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        if len(loss_list) ==0:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if epoch_loss > np.mean(losses):\n",
    "                break\n",
    "    end_time = time.time()\n",
    "    training_time.append(end_time -start_time)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b68ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_event(case_bin):\n",
    "    '''\n",
    "    Generate start event before first event\n",
    "    '''\n",
    "    print(case_bin.event['ts'])\n",
    "    empty_data ={'activity':'Start signal', 'ts':datetime.datetime.strftime(case_bin.event['ts'], '%Y-%m-%d %H:%M:%S')}\n",
    "    start_event = prefix_bin(case_bin.caseid, empty_data)\n",
    "    start_event.set_prefix_length(0)\n",
    "    start_event.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "    start_event.update_truelabel(case_bin.event['activity'])\n",
    "    return start_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca7533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_cases(new_case, test_cases):\n",
    "    test_cases.append(new_case)\n",
    "    if len(test_cases) > test_size:\n",
    "        test_cases.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f83953fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, test_x):\n",
    "    test_x = test_x.cuda()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(test_x)\n",
    "        test_output = test_output.cpu().detach().reshape(-1).numpy()[0]\n",
    "        y_pred = [[1-test_output, test_output]]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe2a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_list(window_data):\n",
    "    variant_window = {}\n",
    "    for t in window_data:\n",
    "        values = [i for i in t.encoded if t.encoded[i]==1]\n",
    "        activities = str(sorted([i for i in values if 'activity' in i]))\n",
    "\n",
    "        if activities not in variant_window.keys():\n",
    "            variant_window[activities] =1\n",
    "        else:\n",
    "            variant_window[activities] +=1\n",
    "    return variant_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6b3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_coverage(train_variant, test_variant):   \n",
    "    coverage_ratio = 0\n",
    "    for t in test_variant.keys():\n",
    "        if t in train_variant.keys():\n",
    "            coverage_ratio += test_variant[t]\n",
    "\n",
    "    return coverage_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c26b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(test_data):\n",
    "    testing_label = {'True':0, 'False':0, '':0}\n",
    "    for t in test_data:\n",
    "        label = t[maximum_prefix-1].true_label\n",
    "        testing_label[label]+=1\n",
    "\n",
    "    label_dist = int(testing_label['True'])/30\n",
    "    return label_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcaf36d-e95a-4d7b-a9dc-23b67370f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_cd(dataset_label, cd_type):\n",
    "    file_path = '../triggered_cd/%s/%s_CD_list.pkl'%(cd_type, dataset_label)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcbb3908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpic2011_1\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [10, 19, 27]}\n",
      "10\n",
      "26.62 % Case finished: 227 Running cases: 59\n",
      "33.27 % Case finished: 257 Running cases: 151\n",
      "39.92 % Case finished: 301 Running cases: 223\n",
      "46.58 % Case finished: 358 Running cases: 286\n",
      "53.23 % Case finished: 394 Running cases: 325\n",
      "59.88 % Case finished: 438 Running cases: 370\n",
      "66.54 % Case finished: 483 Running cases: 410\n",
      "73.19 % Case finished: 545 Running cases: 455\n",
      "79.85 % Case finished: 581 Running cases: 501\n",
      "86.5 % Case finished: 612 Running cases: 535\n",
      "93.15 % Case finished: 683 Running cases: 580\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 817 Running cases: 629\n",
      "19\n",
      "26.62 % Case finished: 227 Running cases: 59\n",
      "33.27 % Case finished: 257 Running cases: 151\n",
      "39.92 % Case finished: 301 Running cases: 223\n",
      "46.58 % Case finished: 358 Running cases: 286\n",
      "53.23 % Case finished: 394 Running cases: 325\n",
      "59.88 % Case finished: 438 Running cases: 370\n",
      "66.54 % Case finished: 483 Running cases: 410\n",
      "73.19 % Case finished: 545 Running cases: 455\n",
      "79.85 % Case finished: 581 Running cases: 501\n",
      "86.5 % Case finished: 612 Running cases: 535\n",
      "93.15 % Case finished: 683 Running cases: 580\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 817 Running cases: 629\n",
      "27\n",
      "26.62 % Case finished: 227 Running cases: 59\n",
      "33.27 % Case finished: 257 Running cases: 151\n",
      "39.92 % Case finished: 301 Running cases: 223\n",
      "46.58 % Case finished: 358 Running cases: 286\n",
      "53.23 % Case finished: 394 Running cases: 325\n",
      "59.88 % Case finished: 438 Running cases: 370\n",
      "66.54 % Case finished: 483 Running cases: 410\n",
      "73.19 % Case finished: 545 Running cases: 455\n",
      "79.85 % Case finished: 581 Running cases: 501\n",
      "86.5 % Case finished: 612 Running cases: 535\n",
      "93.15 % Case finished: 683 Running cases: 580\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 817 Running cases: 629\n",
      "bpic2011_3\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [9, 16, 24]}\n",
      "9\n",
      "26.62 % Case finished: 202 Running cases: 13\n",
      "33.27 % Case finished: 230 Running cases: 105\n",
      "39.92 % Case finished: 276 Running cases: 177\n",
      "46.58 % Case finished: 322 Running cases: 240\n",
      "53.23 % Case finished: 358 Running cases: 279\n",
      "59.88 % Case finished: 394 Running cases: 324\n",
      "66.54 % Case finished: 437 Running cases: 364\n",
      "73.19 % Case finished: 490 Running cases: 409\n",
      "79.85 % Case finished: 538 Running cases: 455\n",
      "86.5 % Case finished: 573 Running cases: 489\n",
      "93.15 % Case finished: 644 Running cases: 534\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 773 Running cases: 583\n",
      "16\n",
      "26.62 % Case finished: 202 Running cases: 13\n",
      "33.27 % Case finished: 230 Running cases: 105\n",
      "39.92 % Case finished: 276 Running cases: 177\n",
      "46.58 % Case finished: 322 Running cases: 240\n",
      "53.23 % Case finished: 358 Running cases: 279\n",
      "59.88 % Case finished: 394 Running cases: 324\n",
      "66.54 % Case finished: 437 Running cases: 364\n",
      "73.19 % Case finished: 490 Running cases: 409\n",
      "79.85 % Case finished: 538 Running cases: 455\n",
      "86.5 % Case finished: 573 Running cases: 489\n",
      "93.15 % Case finished: 644 Running cases: 534\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 773 Running cases: 583\n",
      "24\n",
      "26.62 % Case finished: 202 Running cases: 13\n",
      "33.27 % Case finished: 230 Running cases: 105\n",
      "39.92 % Case finished: 276 Running cases: 177\n",
      "46.58 % Case finished: 322 Running cases: 240\n",
      "53.23 % Case finished: 358 Running cases: 279\n",
      "59.88 % Case finished: 394 Running cases: 324\n",
      "66.54 % Case finished: 437 Running cases: 364\n",
      "73.19 % Case finished: 490 Running cases: 409\n",
      "79.85 % Case finished: 538 Running cases: 455\n",
      "86.5 % Case finished: 573 Running cases: 489\n",
      "93.15 % Case finished: 644 Running cases: 534\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 773 Running cases: 583\n",
      "bpic2011_4\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "33.27 % Case finished: 217 Running cases: 81\n",
      "39.92 % Case finished: 257 Running cases: 153\n",
      "46.58 % Case finished: 312 Running cases: 216\n",
      "53.23 % Case finished: 345 Running cases: 255\n",
      "59.88 % Case finished: 384 Running cases: 300\n",
      "66.54 % Case finished: 425 Running cases: 340\n",
      "73.19 % Case finished: 490 Running cases: 385\n",
      "79.85 % Case finished: 524 Running cases: 431\n",
      "86.5 % Case finished: 560 Running cases: 465\n",
      "93.15 % Case finished: 630 Running cases: 510\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 744 Running cases: 559\n",
      "21\n",
      "33.27 % Case finished: 217 Running cases: 81\n",
      "39.92 % Case finished: 257 Running cases: 153\n",
      "46.58 % Case finished: 312 Running cases: 216\n",
      "53.23 % Case finished: 345 Running cases: 255\n",
      "59.88 % Case finished: 384 Running cases: 300\n",
      "66.54 % Case finished: 425 Running cases: 340\n",
      "73.19 % Case finished: 490 Running cases: 385\n",
      "79.85 % Case finished: 524 Running cases: 431\n",
      "86.5 % Case finished: 560 Running cases: 465\n",
      "93.15 % Case finished: 630 Running cases: 510\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 744 Running cases: 559\n",
      "30\n",
      "33.27 % Case finished: 217 Running cases: 81\n",
      "39.92 % Case finished: 257 Running cases: 153\n",
      "46.58 % Case finished: 312 Running cases: 216\n",
      "53.23 % Case finished: 345 Running cases: 255\n",
      "59.88 % Case finished: 384 Running cases: 300\n",
      "66.54 % Case finished: 425 Running cases: 340\n",
      "73.19 % Case finished: 490 Running cases: 385\n",
      "79.85 % Case finished: 524 Running cases: 431\n",
      "86.5 % Case finished: 560 Running cases: 465\n",
      "93.15 % Case finished: 630 Running cases: 510\n",
      "Triggered1\n",
      "Triggered2\n",
      "99.81 % Case finished: 744 Running cases: 559\n",
      "bpic2015_1\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "19.15 % Case finished: 210 Running cases: 55\n",
      "38.3 % Case finished: 445 Running cases: 304\n",
      "57.45 % Case finished: 661 Running cases: 527\n",
      "Triggered1\n",
      "Triggered2\n",
      "Triggered1\n",
      "Triggered2\n",
      "76.6 % Case finished: 859 Running cases: 703\n",
      "Triggered1\n",
      "Triggered2\n",
      "95.75 % Case finished: 1057 Running cases: 894\n",
      "21\n",
      "19.15 % Case finished: 210 Running cases: 55\n",
      "38.3 % Case finished: 445 Running cases: 304\n",
      "57.45 % Case finished: 661 Running cases: 527\n",
      "Triggered1\n",
      "Triggered2\n",
      "76.6 % Case finished: 859 Running cases: 703\n",
      "Triggered1\n",
      "Triggered2\n",
      "Triggered1\n",
      "Triggered2\n",
      "95.75 % Case finished: 1057 Running cases: 894\n",
      "30\n",
      "19.15 % Case finished: 210 Running cases: 55\n",
      "38.3 % Case finished: 445 Running cases: 304\n",
      "57.45 % Case finished: 661 Running cases: 527\n",
      "Triggered1\n",
      "Triggered2\n",
      "76.6 % Case finished: 859 Running cases: 703\n",
      "Triggered1\n",
      "Triggered2\n",
      "95.75 % Case finished: 1057 Running cases: 894\n",
      "Triggered1\n",
      "Triggered2\n",
      "bpic2015_2\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "45.09 % Case finished: 306 Running cases: 151\n",
      "67.64 % Case finished: 449 Running cases: 306\n",
      "90.18 % Case finished: 590 Running cases: 468\n",
      "Triggered1\n",
      "Triggered2\n",
      "21\n",
      "45.09 % Case finished: 306 Running cases: 151\n",
      "67.64 % Case finished: 449 Running cases: 306\n",
      "90.18 % Case finished: 590 Running cases: 468\n",
      "Triggered1\n",
      "Triggered2\n",
      "30\n",
      "45.09 % Case finished: 306 Running cases: 151\n",
      "67.64 % Case finished: 449 Running cases: 306\n",
      "90.18 % Case finished: 590 Running cases: 468\n",
      "Triggered1\n",
      "Triggered2\n",
      "bpic2015_3\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "16.76 % Case finished: 212 Running cases: 48\n",
      "33.51 % Case finished: 448 Running cases: 286\n",
      "50.27 % Case finished: 675 Running cases: 522\n",
      "Triggered1\n",
      "Triggered2\n",
      "67.02 % Case finished: 883 Running cases: 717\n",
      "Triggered1\n",
      "Triggered2\n",
      "83.78 % Case finished: 1111 Running cases: 944\n",
      "21\n",
      "16.76 % Case finished: 212 Running cases: 48\n",
      "33.51 % Case finished: 448 Running cases: 286\n",
      "50.27 % Case finished: 675 Running cases: 522\n",
      "Triggered1\n",
      "Triggered2\n",
      "67.02 % Case finished: 883 Running cases: 717\n",
      "Triggered1\n",
      "Triggered2\n",
      "83.78 % Case finished: 1111 Running cases: 944\n",
      "30\n",
      "16.76 % Case finished: 212 Running cases: 48\n",
      "33.51 % Case finished: 448 Running cases: 286\n",
      "50.27 % Case finished: 675 Running cases: 522\n",
      "Triggered1\n",
      "Triggered2\n",
      "67.02 % Case finished: 883 Running cases: 717\n",
      "83.78 % Case finished: 1111 Running cases: 944\n",
      "bpic2015_4\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "42.29 % Case finished: 378 Running cases: 227\n",
      "63.43 % Case finished: 554 Running cases: 427\n",
      "84.58 % Case finished: 742 Running cases: 607\n",
      "Triggered1\n",
      "Triggered2\n",
      "21\n",
      "42.29 % Case finished: 378 Running cases: 227\n",
      "63.43 % Case finished: 554 Running cases: 427\n",
      "84.58 % Case finished: 742 Running cases: 607\n",
      "Triggered1\n",
      "Triggered2\n",
      "30\n",
      "42.29 % Case finished: 378 Running cases: 227\n",
      "63.43 % Case finished: 554 Running cases: 427\n",
      "84.58 % Case finished: 742 Running cases: 607\n",
      "Triggered1\n",
      "Triggered2\n",
      "bpic2015_5\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 40, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "33.85 % Case finished: 312 Running cases: 167\n",
      "50.78 % Case finished: 510 Running cases: 350\n",
      "67.7 % Case finished: 685 Running cases: 514\n",
      "84.63 % Case finished: 854 Running cases: 703\n",
      "Triggered1\n",
      "Triggered2\n",
      "21\n",
      "33.85 % Case finished: 312 Running cases: 167\n",
      "50.78 % Case finished: 510 Running cases: 350\n",
      "67.7 % Case finished: 685 Running cases: 514\n",
      "84.63 % Case finished: 854 Running cases: 703\n",
      "Triggered1\n",
      "Triggered2\n",
      "30\n",
      "33.85 % Case finished: 312 Running cases: 167\n",
      "50.78 % Case finished: 510 Running cases: 350\n",
      "67.7 % Case finished: 685 Running cases: 514\n",
      "84.63 % Case finished: 854 Running cases: 703\n",
      "Triggered1\n",
      "Triggered2\n"
     ]
    }
   ],
   "source": [
    "# for dataset_label in ['bpic2015_1', 'bpic2015_4', 'bpic2015_5', 'traffic_fines_1']:\n",
    "for dataset_label in ['bpic2011_1', 'bpic2011_3', 'bpic2011_4', 'bpic2015_1', 'bpic2015_2', 'bpic2015_3', 'bpic2015_4', 'bpic2015_5']:\n",
    "# for dataset_label in ['bpic2012_1', 'bpic2012_2', 'bpic2012_3', 'bpic2017_1', 'bpic2017_2', 'bpic2017_3']:\n",
    "    # Experiment settings\n",
    "    label_condition = False\n",
    "    cd_type = 'prefixtreeCDD'\n",
    "    # cd_type = 'prefixtreeCDD'\n",
    "    print(dataset_label)\n",
    "    \n",
    "    # Invoke parameters for dataset\n",
    "    window_size = 200\n",
    "    test_size = 30\n",
    "    gp = 200\n",
    "    training_rebalance = True\n",
    "    retraining_condition = cd_type\n",
    "    classifier = 'lstm'\n",
    "    retraining_trigger = False\n",
    "    \n",
    "    with open('../dataset_parameters.json','r') as json_file:\n",
    "        parameters = json.load(json_file)[dataset_label]\n",
    "        print(parameters)\n",
    "        key_pair = parameters['key_pair']\n",
    "        catatars = parameters['categorical_attrs']\n",
    "        maximum_prefixs = parameters['maximum_prefix']\n",
    "        prefix_range = parameters['prefix']\n",
    "        \n",
    "    for maximum_prefix in prefix_range:\n",
    "        training_time = []\n",
    "        print(maximum_prefix)\n",
    "        train_window_dict = {}\n",
    "        test_window_dict = {}\n",
    "        cd_list = deque(trigger_cd(dataset_label, cd_type))\n",
    "\n",
    "        dataset_loc = '../DATA/logs/'+ dataset_label +'.csv'\n",
    "        try:\n",
    "            os.makedirs('../result/%s'%(dataset_label))\n",
    "        except:\n",
    "            pass\n",
    "        dataset = stream.iter_csv(\n",
    "                dataset_loc\n",
    "                )\n",
    "        totallength = len(list(dataset))\n",
    "\n",
    "        dataset = stream.iter_csv(\n",
    "                dataset_loc,\n",
    "                # drop=['Complete Timestamp'],\n",
    "                target='outcome'\n",
    "                )\n",
    "        enctype = 'Index-base'\n",
    "\n",
    "        streaming_db ={}\n",
    "        training_windows = sliding_window.training_window(window_size,test_size)\n",
    "        training_models ={}\n",
    "        test_cases = deque()\n",
    "        feature_matrix ={}\n",
    "\n",
    "        save_model = {}\n",
    "        casecount = 0\n",
    "        rowcounter = 0\n",
    "        finished_db ={}\n",
    "        running_case = 0\n",
    "        window_acc_dict = {}\n",
    "        prediction_result = dict()\n",
    "        for i in range(1, maximum_prefix+1): prediction_result[i] = {}\n",
    "        finished_caseid = set()\n",
    "        usedingrace = set()\n",
    "\n",
    "        for x,y in dataset:\n",
    "    #         display_progress(rowcounter, totallength)\n",
    "            rowcounter +=1\n",
    "            # Event stream change dictionary keys\n",
    "            x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "            # if dataset_label !='bpic15':\n",
    "            #      x['ts'] = x['ts'][:-4]\n",
    "\n",
    "            x['outcome'] =y \n",
    "            # Initialize case by prefix length        \n",
    "            caseid = x['caseid']\n",
    "            outcome = x['outcome']\n",
    "        #     progress = x['progress']\n",
    "\n",
    "            x.pop('caseid')\n",
    "            x.pop('outcome')\n",
    "\n",
    "        #     x.pop('progress')\n",
    "\n",
    "            case_bin = prefix_bin(caseid, x)\n",
    "            case_bin.set_enctype(enctype)\n",
    "\n",
    "            if caseid not in list(streaming_db.keys()):\n",
    "                case_bin.set_prefix_length(1)    \n",
    "                streaming_db[caseid] = []\n",
    "            elif caseid in finished_caseid:\n",
    "                pass\n",
    "            else:\n",
    "                case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "            # Encode event and cases and add to DB\n",
    "            case_bin.update_truelabel(outcome)   \n",
    "            case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "            ts = case_bin.event['ts']\n",
    "            streaming_db[caseid].append(case_bin)\n",
    "\n",
    "            # Set current activity as outcome of previous event\n",
    "            if case_bin.prefix_length != 1 and case_bin.caseid not in finished_caseid:\n",
    "                case_bin.prev_enc.update_truelabel(x['activity'])\n",
    "\n",
    "            # First prediction for current event\n",
    "\n",
    "            last_event = case_bin\n",
    "            modelid = 'None'\n",
    "            prediction = 'Not Available'\n",
    "\n",
    "            # Detect label appeared case \n",
    "            if outcome != '' and caseid not in finished_caseid:\n",
    "                usedingrace.add(caseid)\n",
    "                for i in streaming_db[caseid]:\n",
    "                    i.update_truelabel(outcome)\n",
    "                finished_caseid.add(caseid)\n",
    "                # Adding newly finished case to training set.    \n",
    "                casecount +=1\n",
    "                # Grace period to collect feature matrix\n",
    "                if casecount < gp:\n",
    "                    case_length = len(streaming_db[caseid])\n",
    "                    if case_length >= maximum_prefix:\n",
    "                        if 'prefix_%s'%(maximum_prefix) not in list(feature_matrix.keys()):\n",
    "                            feature_matrix['prefix_%s'%(maximum_prefix)]=set()\n",
    "                            training_models['prefix_%s'%(maximum_prefix)] = [0,\n",
    "                                                                       0]\n",
    "                        feature_list = list(streaming_db[caseid][maximum_prefix-1].encoded.keys())\n",
    "                        for x in feature_list: feature_matrix['prefix_%s'%(maximum_prefix)].add(x) \n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if casecount not in train_window_dict.keys(): train_window_dict[casecount] = []\n",
    "        if casecount not in test_window_dict.keys(): test_window_dict[casecount] = []\n",
    "\n",
    "        for caseid in list(usedingrace):\n",
    "            case_length = len(streaming_db[caseid])\n",
    "            if case_length >= maximum_prefix:\n",
    "                x = streaming_db[caseid][maximum_prefix-1]\n",
    "                if x.prefix_length != 0:            \n",
    "                    training_windows.update_window(x)        \n",
    "\n",
    "                update_test_cases(streaming_db[caseid], test_cases)\n",
    "        train_window_dict[casecount].append(copy.deepcopy(training_windows.container))\n",
    "\n",
    "        training_x = []\n",
    "        training_y = []\n",
    "        for pos, i in enumerate(training_windows.container):\n",
    "            x_prefix_length = i.prefix_length \n",
    "            i.encoded = utils.readjustment_training(i.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "            training_x.append(i.encoded)\n",
    "            training_y.append(i.true_label)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        training_y = le.fit_transform(training_y)\n",
    "        training_x = pd.DataFrame.from_dict(training_x)\n",
    "\n",
    "        ###\n",
    "        #Oversampling\n",
    "        ###\n",
    "        n_labels = Counter(training_y)['True']\n",
    "        if n_labels <=2:\n",
    "            pass\n",
    "        elif n_labels>2 and n_labels <=5:\n",
    "            smote = SMOTE(k_neighbors=n_labels-1)\n",
    "            training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "        else:\n",
    "            smote = SMOTE()\n",
    "            training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "\n",
    "        training_models['prefix_%s'%(x_prefix_length)][0] = training_stage((training_x, training_y))\n",
    "        training_models['prefix_%s'%(x_prefix_length)][1] = casecount\n",
    "\n",
    "        prediction_result[maximum_prefix][casecount] = {}\n",
    "        y_truelist = []\n",
    "        y_predlist = []\n",
    "        for case in test_cases:\n",
    "            if len(case) >= maximum_prefix:\n",
    "                x = case[maximum_prefix-1]\n",
    "                if x.prefix_length != 0:            \n",
    "                    model = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "                    current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                    current_event = pd.Series(current_event).to_frame().T\n",
    "                    current_event = Customdataset(current_event).test_preprocessing()\n",
    "                    y_pred = predict_proba(model, current_event)            \n",
    "                    y_truelist.append(x.true_label)\n",
    "                    y_predlist.append(y_pred)\n",
    "                    test_window_dict[casecount].append(x)\n",
    "\n",
    "        prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "        prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "        train_variant = variant_list(train_window_dict[200][0])\n",
    "\n",
    "        '''\n",
    "        Streaming event label prediction start.\n",
    "        - Test and training steps are executed when case finished/ event arrived with label\n",
    "        '''\n",
    "        for i in streaming_db.keys(): usedingrace.add(i)\n",
    "        streaming_db ={}\n",
    "        cdhappend ={}\n",
    "        for i in range(1, maximum_prefix+1): cdhappend[i] = 0\n",
    "\n",
    "        ## Start streaming again\n",
    "        for x,y in dataset:\n",
    "            display_progress(rowcounter, totallength)\n",
    "\n",
    "            rowcounter +=1\n",
    "            # Event stream change dictionary keys\n",
    "            x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "            # if dataset_label !='bpic15':\n",
    "            #     x['ts'] = x['ts'][:-4]\n",
    "\n",
    "            # Check label possible\n",
    "            # x = utils.set_label(x)\n",
    "            x['outcome'] =y \n",
    "            # Initialize case by prefix length\n",
    "            caseid = x['caseid']\n",
    "            outcome = x['outcome']\n",
    "            x.pop('caseid')\n",
    "            x.pop('outcome')\n",
    "\n",
    "            if caseid not in usedingrace:\n",
    "                case_bin = prefix_bin(caseid, x)\n",
    "                case_bin.set_enctype(enctype)\n",
    "\n",
    "                if caseid not in list(streaming_db.keys()):\n",
    "                    case_bin.set_prefix_length(1)    \n",
    "                    streaming_db[caseid] = []\n",
    "                    running_case +=1\n",
    "                elif caseid in finished_caseid:\n",
    "                    pass\n",
    "                else:\n",
    "                    case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                    case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "                # Encode event and cases and add to DB\n",
    "                case_bin.update_truelabel(outcome)   \n",
    "                case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "                ts = case_bin.event['ts']\n",
    "\n",
    "                if case_bin.prefix_length == maximum_prefix:\n",
    "                    case_bin.encoded = utils.readjustment_training(case_bin.encoded, feature_matrix['prefix_%s'%(case_bin.prefix_length)])\n",
    "                streaming_db[caseid].append(case_bin)\n",
    "\n",
    "                # Detect label appeared case \n",
    "                if outcome != '' and caseid not in finished_caseid:\n",
    "                    finished_caseid.add(caseid)\n",
    "\n",
    "                    # Adding newly finished case to training set.\n",
    "                    casecount +=1    \n",
    "\n",
    "                    # Modify encoded attributes of cases with feature matrix\n",
    "                    case_length = len(streaming_db[caseid])\n",
    "                    if case_length >= maximum_prefix:\n",
    "\n",
    "                        streaming_db[caseid][maximum_prefix-1].update_truelabel(outcome)\n",
    "                        update_test_cases(streaming_db[caseid], test_cases)\n",
    "                        x = streaming_db[caseid][maximum_prefix-1].encoded\n",
    "                        prefix_length =streaming_db[caseid][maximum_prefix-1].prefix_length                    \n",
    "\n",
    "                        test_variant = variant_list([i[maximum_prefix-1] for i in test_cases])\n",
    "                        variant_cover = variant_coverage(train_variant, test_variant)/test_size\n",
    "                        label_dist = label_distribution(test_cases)\n",
    "                        training_windows.update_window(streaming_db[caseid][maximum_prefix-1])\n",
    "                        \n",
    "                        if retraining_condition == 'prodrift' or retraining_condition == 'prefixtreeCDD':\n",
    "                            if len(cd_list) ==0:\n",
    "                                retraining_trigger = False\n",
    "                                pass\n",
    "                            else:\n",
    "                                if test_cases[0][0].caseid == cd_list[0]:\n",
    "                                    retraining_trigger = True\n",
    "                                    cd_list.popleft()\n",
    "                                    print('Triggered1')\n",
    "                                else:\n",
    "                                    retraining_trigger = False\n",
    "                                    \n",
    "                        if retraining_condition == 'label':\n",
    "                            if label_dist <= 0.1 or label_dist >=0.9:\n",
    "                                label_condition = True\n",
    "                                print(label_dist, 'Triggered1')\n",
    "                            else:\n",
    "                                label_condition = False\n",
    "\n",
    "                        elif retraining_condition == 'variant':\n",
    "                            if variant_cover <=0.5:\n",
    "                                label_condition = True\n",
    "                            else:\n",
    "                                label_condition = False\n",
    "\n",
    "                        # if label_condition == True and retraining_check == True:\n",
    "                        if retraining_trigger == True:\n",
    "                            print('Triggered2')\n",
    "                            if casecount not in train_window_dict.keys(): train_window_dict[casecount] = []\n",
    "                            train_window_dict[casecount].append(copy.deepcopy(training_windows.container))                       \n",
    "\n",
    "                            x_training = pd.DataFrame.from_dict([i.encoded for i in training_windows.container])\n",
    "                            for i in x_training.columns.values: x_training[i] = x_training[i].fillna(0)\n",
    "                            feature_matrix['prefix_%s'%(maximum_prefix)] = x_training.columns.values\n",
    "\n",
    "                            training_x = []\n",
    "                            training_y = []\n",
    "                            for pos, i in enumerate(training_windows.container):\n",
    "                                x_prefix_length = i.prefix_length \n",
    "                                i.encoded = utils.readjustment_training(i.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                training_x.append(i.encoded)\n",
    "                                training_y.append(i.true_label)\n",
    "\n",
    "                            training_y = le.fit_transform(training_y)\n",
    "                            training_x = pd.DataFrame.from_dict(training_x)\n",
    "                            ###\n",
    "                            #Oversampling\n",
    "                            ###\n",
    "                            n_labels = Counter(training_y)['True']\n",
    "\n",
    "                            if n_labels <=2:\n",
    "                                pass\n",
    "                            elif n_labels>2 and n_labels <=5:\n",
    "                                smote = SMOTE(k_neighbors=n_labels-1)\n",
    "                                training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "                            else:\n",
    "                                smote = SMOTE()\n",
    "                                training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "\n",
    "                            ###\n",
    "                            #Model retraining\n",
    "                            ###\n",
    "                            del model\n",
    "\n",
    "                            training_models['prefix_%s'%(x_prefix_length)][0] = training_stage((training_x, training_y))\n",
    "                            training_models['prefix_%s'%(x_prefix_length)][1] = casecount\n",
    "                            train_variant = variant_list(train_window_dict[casecount][0])\n",
    "                            \n",
    "                            if retraining_condition == 'label':\n",
    "                                train_window_dict = dict()\n",
    "                                \n",
    "    #                         save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "                            save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = 0\n",
    "\n",
    "                    y_truelist = []\n",
    "                    y_predlist = []\n",
    "                    if casecount ==200:\n",
    "                        break\n",
    "                    if casecount not in test_window_dict.keys():\n",
    "                        test_window_dict[casecount] = []\n",
    "\n",
    "                    for case in test_cases:\n",
    "                        if len(case) >= maximum_prefix:\n",
    "                            x = case[maximum_prefix-1]\n",
    "                            if x.prefix_length != 0:\n",
    "                                length = x.prefix_length\n",
    "                                current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                current_event = pd.Series(current_event).to_frame().T\n",
    "                                current_event = Customdataset(current_event).test_preprocessing()\n",
    "                                model = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "                                with torch.no_grad():\n",
    "                #                     label_classes = training_models['detector_window_%s'%(last_event.prefix_length)][2]\n",
    "                                    current_event = current_event.cuda()\n",
    "                                    test_output = model(current_event)\n",
    "                                    test_output = test_output.cpu().detach().reshape(-1).numpy()[0]\n",
    "                                    y_pred = [[1-test_output, test_output]]\n",
    "\n",
    "                                y_truelist.append(x.true_label)\n",
    "                                y_predlist.append(y_pred)\n",
    "                                test_window_dict[casecount].append(x)\n",
    "                                if casecount != gp:\n",
    "                                    c_id = [x.caseid for x in test_window_dict[casecount-1]]\n",
    "\n",
    "                                if x.caseid not in c_id:\n",
    "                                    model = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "                                    y_pred = predict_proba(model, current_event)\n",
    "                                else:\n",
    "                                    y_pred = prediction_result[maximum_prefix][casecount-1]['y_pred'][c_id.index(x.caseid)]\n",
    "                                y_truelist.append(x.true_label)\n",
    "                                y_predlist.append(y_pred)\n",
    "\n",
    "                                if casecount not in test_window_dict.keys():\n",
    "                                    test_window_dict[casecount] = []\n",
    "                                test_window_dict[casecount].append(x)\n",
    "                    prediction_result[maximum_prefix][casecount] = {}\n",
    "                    prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "                    prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "    #                 if 'b1' not in caseid and cdhappend[maximum_prefix] == 0:\n",
    "    #                     cdhappend[maximum_prefix] = model_update_count\n",
    "\n",
    "        try:\n",
    "            os.makedirs('../result/%s/%s/Finished cases/Trigger %s'%(dataset_label, classifier, retraining_condition))\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "#         with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s training window retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "#             pkl.dump(train_window_dict, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s test window retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(test_window_dict, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s update retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(prediction_result, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s model.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(save_model, f)\n",
    "        with gzip.open('../result/time/%s_%s_%s_%s_trainingtime.pkl'%(dataset_label, classifier, maximum_prefix, retraining_condition), 'wb') as f:\n",
    "            pkl.dump(training_time, f)\n",
    "            \n",
    "        # training_time.append(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ac8e2-872a-4629-886e-2b60e13e060d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
