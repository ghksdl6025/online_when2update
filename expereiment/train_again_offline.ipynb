{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from river import stream,tree,metrics\n",
    "from encoding import prefix_bin\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import os,json\n",
    "import datetime\n",
    "from collections import deque, Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import xgboost as xgb\n",
    "\n",
    "import gzip\n",
    "\n",
    "import datetime, time\n",
    "import utils\n",
    "import sliding_window\n",
    "import psutil\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(row_counting, total_length, interval=10000):\n",
    "    if rowcounter%interval == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running cases: %s'%(len(streaming_db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_cases(new_case, test_cases):\n",
    "    test_cases.append(new_case)\n",
    "    if len(test_cases) > test_size:\n",
    "        test_cases.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_list(window_data):\n",
    "    variant_window = {}\n",
    "    for t in window_data:\n",
    "        values = [i for i in t.encoded if t.encoded[i]==1]\n",
    "        activities = str(sorted([i for i in values if 'activity' in i]))\n",
    "\n",
    "        if activities not in variant_window.keys():\n",
    "            variant_window[activities] =1\n",
    "        else:\n",
    "            variant_window[activities] +=1\n",
    "    return variant_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_coverage(train_variant, test_variant):   \n",
    "    coverage_ratio = 0\n",
    "    for t in test_variant.keys():\n",
    "        if t in train_variant.keys():\n",
    "            coverage_ratio += test_variant[t]\n",
    "\n",
    "    return coverage_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(test_data):\n",
    "    testing_label = {'True':0, 'False':0, '':0}\n",
    "    for t in test_data:\n",
    "        label = t[maximum_prefix-1].true_label\n",
    "        testing_label[label]+=1\n",
    "\n",
    "    label_dist = int(testing_label['True'])/30\n",
    "    return label_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production_Data\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 14, 'prefix': [7, 12, 18]}\n",
      "7\n",
      "12\n",
      "18\n",
      "sepsis_cases_1\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 15, 'prefix': [8, 16, 23]}\n",
      "8\n",
      "71.93 % Case finished: 535 Running cases: 350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 351\u001b[0m\n\u001b[0;32m    348\u001b[0m     c_id \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcaseid \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m test_window_dict[casecount\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcaseid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m c_id:\n\u001b[1;32m--> 351\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprefix_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_prefix_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_event\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m prediction_result[maximum_prefix][casecount\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m'\u001b[39m][c_id\u001b[38;5;241m.\u001b[39mindex(x\u001b[38;5;241m.\u001b[39mcaseid)]\n",
      "File \u001b[1;32mC:\\Python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:949\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    946\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_partition_estimators\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# avoid storing the output of every estimator by summing them here\u001b[39;00m\n\u001b[0;32m    952\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    953\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m    955\u001b[0m ]\n",
      "File \u001b[1;32mC:\\Python\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:172\u001b[0m, in \u001b[0;36m_partition_estimators\u001b[1;34m(n_estimators, n_jobs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private function used to partition estimators between jobs.\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Compute the number of jobs\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[43meffective_n_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m, n_estimators)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Partition estimators between jobs\u001b[39;00m\n\u001b[0;32m    175\u001b[0m n_estimators_per_job \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(n_jobs, n_estimators \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_jobs, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python\\Lib\\site-packages\\joblib\\parallel.py:942\u001b[0m, in \u001b[0;36meffective_n_jobs\u001b[1;34m(n_jobs)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m backend_n_jobs\n\u001b[1;32m--> 942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_n_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\Lib\\site-packages\\joblib\\_parallel_backends.py:583\u001b[0m, in \u001b[0;36mLokyBackend.effective_n_jobs\u001b[1;34m(self, n_jobs)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n_jobs \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 583\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m n_jobs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_jobs\n",
      "File \u001b[1;32mC:\\Python\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:119\u001b[0m, in \u001b[0;36mcpu_count\u001b[1;34m(only_physical_cores)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# On Windows, attempting to use more than 61 CPUs would result in a\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# OS-level error. See https://bugs.python.org/issue26903. According to\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# https://learn.microsoft.com/en-us/windows/win32/procthread/processor-groups\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# it might be possible to go beyond with a lot of extra work but this\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# does not look easy.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     os_cpu_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(os_cpu_count, _MAX_WINDOWS_WORKERS)\n\u001b[1;32m--> 119\u001b[0m cpu_count_user \u001b[38;5;241m=\u001b[39m \u001b[43m_cpu_count_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos_cpu_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m aggregate_cpu_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmin\u001b[39m(os_cpu_count, cpu_count_user), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_physical_cores:\n",
      "File \u001b[1;32mC:\\Python\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:224\u001b[0m, in \u001b[0;36m_cpu_count_user\u001b[1;34m(os_cpu_count)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Number of user defined available CPUs\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m cpu_count_affinity \u001b[38;5;241m=\u001b[39m _cpu_count_affinity(os_cpu_count)\n\u001b[1;32m--> 224\u001b[0m cpu_count_cgroup \u001b[38;5;241m=\u001b[39m \u001b[43m_cpu_count_cgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos_cpu_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# User defined soft-limit passed as a loky specific environment variable.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m cpu_count_loky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOKY_MAX_CPU_COUNT\u001b[39m\u001b[38;5;124m\"\u001b[39m, os_cpu_count))\n",
      "File \u001b[1;32mC:\\Python\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:158\u001b[0m, in \u001b[0;36m_cpu_count_cgroup\u001b[1;34m(os_cpu_count)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cpu_max_fname) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[0;32m    157\u001b[0m         cpu_quota_us, cpu_period_us \u001b[38;5;241m=\u001b[39m fh\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfs_quota_fname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cfs_period_fname):\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# cgroup v1\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# https://www.kernel.org/doc/html/latest/scheduler/sched-bwc.html#management\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cfs_quota_fname) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[0;32m    162\u001b[0m         cpu_quota_us \u001b[38;5;241m=\u001b[39m fh\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m<frozen genericpath>:16\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_label in ['Production_Data','sepsis_cases_1','sepsis_cases_2','sepsis_cases_3','traffic_fines_1','hospital_billing_1',\n",
    "                      'hospital_billing_2']:\n",
    "# for dataset_label in ['bpic2017_3','IRO5K' ,'OIR5K']:\n",
    "    # Experiment settings\n",
    "    label_condition = False\n",
    "    print(dataset_label)\n",
    "    './DATA/logs/synthetic_log_bc1.csv'\n",
    "#     dataset_label = 'bpic2015_1'\n",
    "\n",
    "    # Invoke parameters for dataset\n",
    "    window_size = 200\n",
    "    test_size = 30\n",
    "    gp = 200\n",
    "    training_rebalance = True\n",
    "    retraining_condition = 'label'\n",
    "    classifier = 'rf'\n",
    "\n",
    "    with open('../dataset_parameters.json','r') as json_file:\n",
    "        parameters = json.load(json_file)[dataset_label]\n",
    "        print(parameters)\n",
    "        key_pair = parameters['key_pair']\n",
    "        catatars = parameters['categorical_attrs']\n",
    "        maximum_prefixs = parameters['maximum_prefix']\n",
    "        prefix_range = parameters['prefix']\n",
    "\n",
    "    for maximum_prefix in prefix_range:\n",
    "        training_time = []\n",
    "        print(maximum_prefix)\n",
    "        retraining_check = True\n",
    "        train_window_dict = {}\n",
    "        test_window_dict = {}\n",
    "        dataset_loc = '../DATA/logs/'+ dataset_label +'.csv'\n",
    "        try:\n",
    "            os.makedirs('../result/%s'%(dataset_label))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Set streaming classifier\n",
    "        if classifier == 'rf':\n",
    "            streaming_classifier = RandomForestClassifier(n_estimators = 10, n_jobs = -1, random_state=500)\n",
    "        elif classifier == 'xgb':\n",
    "            streaming_classifier = xgb.XGBClassifier(n_estimators = 10, learning_rate=0.01, verbosity =0, random_state=500)     \n",
    "\n",
    "        dataset = stream.iter_csv(\n",
    "                dataset_loc\n",
    "                )\n",
    "\n",
    "        totallength = len(list(dataset))\n",
    "\n",
    "        dataset = stream.iter_csv(\n",
    "                dataset_loc,\n",
    "                # drop=['Complete Timestamp'],\n",
    "                target='outcome'\n",
    "                )\n",
    "        enctype = 'Index-base'\n",
    "\n",
    "        streaming_db ={}\n",
    "        training_windows = sliding_window.training_window(window_size,test_size)\n",
    "        training_models ={}\n",
    "        test_cases = deque()\n",
    "        feature_matrix ={}\n",
    "\n",
    "        save_model = {}\n",
    "        casecount = 0\n",
    "        rowcounter = 0\n",
    "        finished_db ={}\n",
    "        running_case = 0\n",
    "        window_acc_dict = {}\n",
    "        prediction_result = dict()\n",
    "        for i in range(1, maximum_prefix+1): prediction_result[i] = {}\n",
    "        finished_caseid = set()\n",
    "        usedingrace = set()\n",
    "\n",
    "\n",
    "        '''\n",
    "        Before test and training streaming event predictive monitoring, grace period is preceded to initialize model by prefix length\n",
    "        and obtain feature matrix to transform future events\n",
    "        '''\n",
    "        for x,y in dataset:\n",
    "    #         display_progress(rowcounter, totallength)\n",
    "            rowcounter +=1\n",
    "            # Event stream change dictionary keys\n",
    "            x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "            # if dataset_label !='bpic15':\n",
    "            #     x['ts'] = x['ts'][:-4]\n",
    "\n",
    "            x['outcome'] =y \n",
    "            # Initialize case by prefix length        \n",
    "            caseid = x['caseid']\n",
    "            outcome = x['outcome']\n",
    "        #     progress = x['progress']\n",
    "\n",
    "            x.pop('caseid')\n",
    "            x.pop('outcome')\n",
    "\n",
    "        #     x.pop('progress')\n",
    "\n",
    "            case_bin = prefix_bin(caseid, x)\n",
    "            case_bin.set_enctype(enctype)\n",
    "\n",
    "            if caseid not in list(streaming_db.keys()):\n",
    "                case_bin.set_prefix_length(1)    \n",
    "                streaming_db[caseid] = []\n",
    "            elif caseid in finished_caseid:\n",
    "                pass\n",
    "            else:\n",
    "                case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "            # Encode event and cases and add to DB\n",
    "            case_bin.update_truelabel(outcome)   \n",
    "            case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "            ts = case_bin.event['ts']\n",
    "            streaming_db[caseid].append(case_bin)\n",
    "            # Detect label appeared case \n",
    "            if outcome != '' and caseid not in finished_caseid:\n",
    "                usedingrace.add(caseid)\n",
    "                for i in streaming_db[caseid]:\n",
    "                    i.update_truelabel(outcome)\n",
    "                finished_caseid.add(caseid)\n",
    "                # Adding newly finished case to training set.    \n",
    "                casecount +=1\n",
    "                # Grace period to collect feature matrix\n",
    "                if casecount < gp:\n",
    "                    case_length = len(streaming_db[caseid])\n",
    "                    if case_length >= maximum_prefix:\n",
    "                        if 'prefix_%s'%(maximum_prefix) not in list(feature_matrix.keys()):\n",
    "                            feature_matrix['prefix_%s'%(maximum_prefix)]=set()\n",
    "                            training_models['prefix_%s'%(maximum_prefix)] = [streaming_classifier,\n",
    "                                                                       0]\n",
    "                        feature_list = list(streaming_db[caseid][maximum_prefix-1].encoded.keys())\n",
    "                        for x in feature_list: feature_matrix['prefix_%s'%(maximum_prefix)].add(x) \n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if casecount not in train_window_dict.keys(): train_window_dict[casecount] = []\n",
    "        if casecount not in test_window_dict.keys(): test_window_dict[casecount] = []\n",
    "\n",
    "        for caseid in list(usedingrace):\n",
    "            case_length = len(streaming_db[caseid])\n",
    "            if case_length >= maximum_prefix:\n",
    "                x = streaming_db[caseid][maximum_prefix-1]\n",
    "                if x.prefix_length != 0:            \n",
    "                    training_windows.update_window(x)\n",
    "\n",
    "                update_test_cases(streaming_db[caseid], test_cases)\n",
    "        train_window_dict[casecount].append(copy.deepcopy(training_windows.container))\n",
    "\n",
    "        training_x = []\n",
    "        training_y = []\n",
    "        for pos, i in enumerate(training_windows.container):\n",
    "            x_prefix_length = i.prefix_length \n",
    "            i.encoded = utils.readjustment_training(i.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "            training_x.append(i.encoded)\n",
    "            training_y.append(i.true_label)\n",
    "\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        training_y = le.fit_transform(training_y)\n",
    "        training_x = pd.DataFrame.from_dict(training_x)\n",
    "\n",
    "        ###\n",
    "        #Oversampling\n",
    "        ###\n",
    "        n_labels = Counter(training_y)['True']\n",
    "\n",
    "        if n_labels <=2:\n",
    "            pass\n",
    "        elif n_labels>2 and n_labels <=5:\n",
    "            smote = SMOTE(k_neighbors=n_labels-1)\n",
    "            training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "        else:\n",
    "            smote = SMOTE()\n",
    "            training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "\n",
    "        start_time = time.time()\n",
    "        training_models['prefix_%s'%(x_prefix_length)][0].fit(training_x, training_y)\n",
    "        training_models['prefix_%s'%(x_prefix_length)][1] =casecount\n",
    "        end_time = time.time()\n",
    "        training_time.append(end_time-start_time)\n",
    "        \n",
    "        prediction_result[maximum_prefix][casecount] = {}\n",
    "        y_truelist = []\n",
    "        y_predlist = []\n",
    "        for case in test_cases:\n",
    "            if len(case) >= maximum_prefix:\n",
    "                x = case[maximum_prefix-1]\n",
    "                if x.prefix_length != 0:            \n",
    "                    model = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "                    current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                    current_event = pd.Series(current_event).to_frame().T\n",
    "                    y_pred = training_models['prefix_%s'%(x_prefix_length)][0].predict_proba(current_event)\n",
    "                    y_truelist.append(x.true_label)\n",
    "                    y_predlist.append(y_pred)\n",
    "                    test_window_dict[casecount].append(x)\n",
    "        prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "        prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "\n",
    "        train_variant = variant_list(train_window_dict[200][0])\n",
    "        save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "        '''\n",
    "        Streaming event label prediction start.\n",
    "        - Test and training steps are executed when case finished/ event arrived with label\n",
    "        '''\n",
    "        for i in streaming_db.keys(): usedingrace.add(i)\n",
    "        streaming_db ={}\n",
    "        cdhappend ={}\n",
    "        for i in range(1, maximum_prefix+1): cdhappend[i] = 0\n",
    "\n",
    "        for x,y in dataset:\n",
    "            display_progress(rowcounter, totallength)\n",
    "\n",
    "            rowcounter +=1\n",
    "            # Event stream change dictionary keys\n",
    "            x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "            # if dataset_label !='bpic15':\n",
    "            #     x['ts'] = x['ts'][:-4]\n",
    "\n",
    "            # Check label possible\n",
    "            # x = utils.set_label(x)\n",
    "            x['outcome'] =y \n",
    "            # Initialize case by prefix length\n",
    "            caseid = x['caseid']\n",
    "            outcome = x['outcome']\n",
    "            x.pop('caseid')\n",
    "            x.pop('outcome')\n",
    "\n",
    "            if caseid not in usedingrace:\n",
    "                case_bin = prefix_bin(caseid, x)\n",
    "                case_bin.set_enctype(enctype)\n",
    "\n",
    "                if caseid not in list(streaming_db.keys()):\n",
    "                    case_bin.set_prefix_length(1)    \n",
    "                    streaming_db[caseid] = []\n",
    "                    running_case +=1\n",
    "                elif caseid in finished_caseid:\n",
    "                    pass\n",
    "                else:\n",
    "                    case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                    case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "                # Encode event and cases and add to DB\n",
    "                case_bin.update_truelabel(outcome)   \n",
    "                case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "                ts = case_bin.event['ts']\n",
    "\n",
    "                if case_bin.prefix_length == maximum_prefix:\n",
    "                    case_bin.encoded = utils.readjustment_training(case_bin.encoded, feature_matrix['prefix_%s'%(case_bin.prefix_length)])\n",
    "                streaming_db[caseid].append(case_bin)\n",
    "\n",
    "                # Detect label appeared case \n",
    "                if outcome != '' and caseid not in finished_caseid:\n",
    "                    finished_caseid.add(caseid)\n",
    "\n",
    "                    # Adding newly finished case to training set.\n",
    "                    casecount +=1    \n",
    "\n",
    "                    # Modify encoded attributes of cases with feature matrix\n",
    "                    case_length = len(streaming_db[caseid])\n",
    "                    if case_length >= maximum_prefix:\n",
    "\n",
    "                        update_test_cases(streaming_db[caseid], test_cases)\n",
    "                        streaming_db[caseid][maximum_prefix-1].update_truelabel(outcome)\n",
    "                        x = streaming_db[caseid][maximum_prefix-1].encoded\n",
    "                        prefix_length =streaming_db[caseid][maximum_prefix-1].prefix_length                    \n",
    "\n",
    "                        test_variant = variant_list([i[maximum_prefix-1] for i in test_cases])\n",
    "                        variant_cover = variant_coverage(train_variant, test_variant)/test_size\n",
    "                        label_dist = label_distribution(test_cases)\n",
    "                        training_windows.update_window(streaming_db[caseid][maximum_prefix-1])\n",
    "                        if retraining_condition == 'label':\n",
    "                            if label_dist <= 0.1 or label_dist >=0.9:\n",
    "                                label_condition = True\n",
    "                            else:\n",
    "                                label_condition = False\n",
    "\n",
    "                        elif retraining_condition == 'variant':\n",
    "                            if variant_cover <=0.5:\n",
    "                                label_condition = True\n",
    "                            else:\n",
    "                                label_condition = False\n",
    "\n",
    "                        if label_condition == True and retraining_check == True:\n",
    "\n",
    "                            if casecount not in train_window_dict.keys(): train_window_dict[casecount] = []\n",
    "                            train_window_dict[casecount].append(copy.deepcopy(training_windows.container))                       \n",
    "\n",
    "                            x_training = pd.DataFrame.from_dict([i.encoded for i in training_windows.container])\n",
    "                            for i in x_training.columns.values: x_training[i] = x_training[i].fillna(0)\n",
    "                            feature_matrix['prefix_%s'%(maximum_prefix)] = x_training.columns.values\n",
    "\n",
    "                            training_x = []\n",
    "                            training_y = []\n",
    "                            for pos, i in enumerate(training_windows.container):\n",
    "                                x_prefix_length = i.prefix_length \n",
    "                                i.encoded = utils.readjustment_training(i.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                training_x.append(i.encoded)\n",
    "                                training_y.append(i.true_label)\n",
    "                            training_y = le.fit_transform(training_y)\n",
    "                            training_x = pd.DataFrame.from_dict(training_x)\n",
    "\n",
    "                            ###\n",
    "                            #Oversampling\n",
    "                            ###\n",
    "                            n_labels = Counter(training_y)['True']\n",
    "\n",
    "                            if n_labels <=2:\n",
    "                                pass\n",
    "                            elif n_labels>2 and n_labels <=5:\n",
    "                                smote = SMOTE(k_neighbors=n_labels-1)\n",
    "                                training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "                            else:\n",
    "                                smote = SMOTE()\n",
    "                                training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "                                                   ###\n",
    "                            #Model retraining\n",
    "                            ###\n",
    "                            start_time = time.time()\n",
    "                            training_models['prefix_%s'%(maximum_prefix)][0] = streaming_classifier\n",
    "                            training_models['prefix_%s'%(maximum_prefix)][0].fit(training_x, training_y)\n",
    "                            training_models['prefix_%s'%(x_prefix_length)][1] =casecount\n",
    "                            train_variant = variant_list(train_window_dict[casecount][0])\n",
    "                            end_time = time.time()\n",
    "                            training_time.append(end_time-start_time)\n",
    "                            \n",
    "                            if retraining_condition == 'label':\n",
    "                                train_window_dict = dict()\n",
    "\n",
    "                            save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = training_models['prefix_%s'%(x_prefix_length)]\n",
    "#                             save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = 0\n",
    "                    y_truelist = []\n",
    "                    y_predlist = []\n",
    "\n",
    "                    if casecount not in test_window_dict.keys():\n",
    "                        test_window_dict[casecount] = []\n",
    "\n",
    "                    for case in test_cases:\n",
    "                        if len(case) >= maximum_prefix:\n",
    "                            x = case[maximum_prefix-1]\n",
    "                            if x.prefix_length != 0:\n",
    "                                length = x.prefix_length\n",
    "                                current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                current_event = pd.Series(current_event).to_frame().T\n",
    "\n",
    "                                if casecount != gp:\n",
    "                                    c_id = [x.caseid for x in test_window_dict[casecount-1]]\n",
    "\n",
    "                                if x.caseid not in c_id:\n",
    "                                    y_pred = training_models['prefix_%s'%(x_prefix_length)][0].predict_proba(current_event)\n",
    "                                else:\n",
    "                                    y_pred = prediction_result[maximum_prefix][casecount-1]['y_pred'][c_id.index(x.caseid)]\n",
    "                                y_truelist.append(x.true_label)\n",
    "                                y_predlist.append(y_pred)\n",
    "\n",
    "                                if casecount not in test_window_dict.keys(): test_window_dict[casecount] = []\n",
    "                                test_window_dict[casecount].append(x)\n",
    "                        prediction_result[maximum_prefix][casecount] = {}\n",
    "                        prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "                        prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "    #                 if 'b1' not in caseid and cdhappend[maximum_prefix] == 0:\n",
    "    #                     cdhappend[maximum_prefix] = model_update_count\n",
    "\n",
    "        try:\n",
    "            os.makedirs('../result/%s/%s/Finished cases/Trigger %s'%(dataset_label, classifier, retraining_condition))\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "#         with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s training window retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "#             pkl.dump(train_window_dict, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s test window retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(test_window_dict, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s update retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(prediction_result, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s model.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(save_model, f)\n",
    "        with gzip.open('../result/time/%s_%s_%s_%s_trainingtime.pkl'%(dataset_label, classifier, maximum_prefix, retraining_condition), 'wb') as f:\n",
    "            pkl.dump(training_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.01, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=10, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=500,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.01, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=10, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=500,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=10, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=500,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_models['prefix_%s'%(x_prefix_length)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.017293691635131836, 0.019320011138916016, 0.019515037536621094]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in test_cases:\n",
    "    if i[0].true_label != str(True) and i[0].true_label != str(False):\n",
    "        print(i[0].true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../result/IRO5K/xgb/Finished cases/train_rebalance_True prefix_12 update.pkl'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'../result/%s/%s/Finished cases/train_rebalance_%s prefix_%s update.pkl'%(dataset_label, classifier, str(training_rebalance), maximum_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
