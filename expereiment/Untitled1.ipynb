{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0512cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from river import stream,tree,metrics\n",
    "from encoding import prefix_bin\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import os,json\n",
    "import datetime\n",
    "from collections import deque, Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import xgboost as xgb\n",
    "\n",
    "import gzip\n",
    "\n",
    "import datetime, time\n",
    "import utils\n",
    "import sliding_window\n",
    "import psutil\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0976b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(row_counting, total_length, interval=20000):\n",
    "    if rowcounter%interval == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running cases: %s'%(len(streaming_db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3f9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_cases(new_case, test_cases):\n",
    "    test_cases.append(new_case)\n",
    "    if len(test_cases) > test_size:\n",
    "        test_cases.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e637cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_list(window_data):\n",
    "    variant_window = {}\n",
    "    for t in window_data:\n",
    "        values = [i for i in t.encoded if t.encoded[i]==1]\n",
    "        activities = str(sorted([i for i in values if 'activity' in i]))\n",
    "\n",
    "        if activities not in variant_window.keys():\n",
    "            variant_window[activities] =1\n",
    "        else:\n",
    "            variant_window[activities] +=1\n",
    "    return variant_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17763436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variant_coverage(train_variant, test_variant):   \n",
    "    coverage_ratio = 0\n",
    "    for t in test_variant.keys():\n",
    "        if t in train_variant.keys():\n",
    "            coverage_ratio += test_variant[t]\n",
    "\n",
    "    return coverage_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7832bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(test_data):\n",
    "    testing_label = {'True':0, 'False':0, '':0}\n",
    "    for t in test_data:\n",
    "        label = t[maximum_prefix-1].true_label\n",
    "        testing_label[label]+=1\n",
    "\n",
    "    label_dist = int(testing_label['True'])/30\n",
    "    return label_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff335b4-6500-4736-81bb-c8dd8a53d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_cd(dataset_label, cd_type):\n",
    "    file_path = '../triggered_cd/%s/%s_CD_list.pkl'%(cd_type, dataset_label)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0cea6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpic2012_1\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 36, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "Triggered1\n",
      "Triggered2\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "21\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "Triggered1\n",
      "Triggered2\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "30\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "Triggered1\n",
      "Triggered2\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "bpic2012_2\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 36, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "Triggered1\n",
      "Triggered2\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "21\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "Triggered1\n",
      "Triggered2\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "30\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "Triggered1\n",
      "Triggered2\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "bpic2012_3\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 36, 'prefix': [11, 21, 30]}\n",
      "11\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "Triggered1\n",
      "Triggered2\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "21\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "Triggered1\n",
      "Triggered2\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "30\n",
      "10.45 % Case finished: 200 Running cases: 66\n",
      "20.9 % Case finished: 330 Running cases: 598\n",
      "31.35 % Case finished: 719 Running cases: 1093\n",
      "Triggered1\n",
      "Triggered2\n",
      "41.8 % Case finished: 1204 Running cases: 1538\n",
      "52.25 % Case finished: 1692 Running cases: 1970\n",
      "62.7 % Case finished: 2071 Running cases: 2552\n",
      "73.15 % Case finished: 2525 Running cases: 3019\n",
      "83.6 % Case finished: 2993 Running cases: 3481\n",
      "94.05 % Case finished: 3602 Running cases: 3876\n",
      "bpic2017_1\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 36, 'prefix': [6, 11, 15]}\n",
      "6\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "11\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "15\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "bpic2017_2\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 36, 'prefix': [6, 11, 15]}\n",
      "6\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "Triggered1\n",
      "Triggered2\n",
      "11\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "Triggered1\n",
      "Triggered2\n",
      "15\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "Triggered1\n",
      "Triggered2\n",
      "bpic2017_3\n",
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Resource': 'resource', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity', 'resource'], 'maximum_prefix': 36, 'prefix': [6, 11, 15]}\n",
      "6\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "Triggered1\n",
      "Triggered2\n",
      "11\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "Triggered1\n",
      "Triggered2\n",
      "15\n",
      "25.27 % Case finished: 348 Running cases: 237\n",
      "50.55 % Case finished: 798 Running cases: 728\n",
      "Triggered1\n",
      "Triggered2\n",
      "75.82 % Case finished: 1317 Running cases: 1264\n",
      "Triggered1\n",
      "Triggered2\n"
     ]
    }
   ],
   "source": [
    "# for dataset_label in ['Production_Data','sepsis_cases_1','sepsis_cases_2','sepsis_cases_3','traffic_fines_1','hospital_billing_1',\n",
    "#                       'hospital_billing_2']:\n",
    "\n",
    "# ['bpic2011_1', 'bpic2011_3', 'bpic2011_4', 'bpic2015_1', 'bpic2015_2', 'bpic2015_3', 'bpic2015_4', 'bpic2015_5']\n",
    "# ['bpic2012_1', 'bpic2012_2', 'bpic2012_3', 'bpic2017_1', 'bpic2017_2', 'bpic2017_3']\n",
    "\n",
    "for dataset_label in ['bpic2012_1', 'bpic2012_2', 'bpic2012_3', 'bpic2017_1', 'bpic2017_2', 'bpic2017_3']:\n",
    "\n",
    "    # Experiment settings\n",
    "    label_condition = False\n",
    "    cd_type = 'prefixtreeCDD'\n",
    "    # cd_type = 'prefixtreeCDD'\n",
    "    \n",
    "    print(dataset_label)\n",
    "    './DATA/logs/synthetic_log_bc1.csv'\n",
    "    \n",
    "    # Invoke parameters for dataset\n",
    "    window_size = 200\n",
    "    test_size = 30\n",
    "    gp = 200\n",
    "    training_rebalance = True\n",
    "    retraining_condition = cd_type\n",
    "    classifier = 'rf'\n",
    "    retraining_trigger = False\n",
    "    with open('../dataset_parameters.json','r') as json_file:\n",
    "        parameters = json.load(json_file)[dataset_label]\n",
    "        print(parameters)\n",
    "        key_pair = parameters['key_pair']\n",
    "        catatars = parameters['categorical_attrs']\n",
    "        maximum_prefixs = parameters['maximum_prefix']\n",
    "        prefix_range = parameters['prefix']\n",
    "\n",
    "    for maximum_prefix in prefix_range:\n",
    "        training_time = []\n",
    "        print(maximum_prefix)\n",
    "        retraining_check = True\n",
    "        train_window_dict = {}\n",
    "        test_window_dict = {}\n",
    "        dataset_loc = '../DATA/logs/'+ dataset_label +'.csv'\n",
    "        cd_list = deque(trigger_cd(dataset_label, cd_type))\n",
    "        try:\n",
    "            os.makedirs('../result/%s'%(dataset_label))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Set streaming classifier\n",
    "        if classifier == 'rf':\n",
    "            streaming_classifier = RandomForestClassifier(n_estimators = 10, n_jobs = -1, random_state=500)\n",
    "        elif classifier == 'xgb':\n",
    "            streaming_classifier = xgb.XGBClassifier(n_estimators = 10, learning_rate=0.01, verbosity =0, random_state=500)     \n",
    "\n",
    "        dataset = stream.iter_csv(\n",
    "                dataset_loc\n",
    "                )\n",
    "\n",
    "        totallength = len(list(dataset))\n",
    "\n",
    "        dataset = stream.iter_csv(\n",
    "                dataset_loc,\n",
    "                # drop=['Complete Timestamp'],\n",
    "                target='outcome'\n",
    "                )\n",
    "        enctype = 'Index-base'\n",
    "\n",
    "        streaming_db ={}\n",
    "        training_windows = sliding_window.training_window(window_size,test_size)\n",
    "        training_models ={}\n",
    "        test_cases = deque()\n",
    "        feature_matrix ={}\n",
    "\n",
    "        save_model = {}\n",
    "        casecount = 0\n",
    "        rowcounter = 0\n",
    "        finished_db ={}\n",
    "        running_case = 0\n",
    "        window_acc_dict = {}\n",
    "        prediction_result = dict()\n",
    "        for i in range(1, maximum_prefix+1): prediction_result[i] = {}\n",
    "        finished_caseid = set()\n",
    "        usedingrace = set()\n",
    "\n",
    "\n",
    "        '''\n",
    "        Before test and training streaming event predictive monitoring, grace period is preceded to initialize model by prefix length\n",
    "        and obtain feature matrix to transform future events\n",
    "        '''\n",
    "        for x,y in dataset:\n",
    "    #         display_progress(rowcounter, totallength)\n",
    "            rowcounter +=1\n",
    "            # Event stream change dictionary keys\n",
    "            x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "            if dataset_label !='bpic15':\n",
    "                x['ts'] = x['ts'][:-4]\n",
    "\n",
    "            x['outcome'] =y \n",
    "            # Initialize case by prefix length        \n",
    "            caseid = x['caseid']\n",
    "            outcome = x['outcome']\n",
    "        #     progress = x['progress']\n",
    "\n",
    "            x.pop('caseid')\n",
    "            x.pop('outcome')\n",
    "\n",
    "        #     x.pop('progress')\n",
    "\n",
    "            case_bin = prefix_bin(caseid, x)\n",
    "            case_bin.set_enctype(enctype)\n",
    "\n",
    "            if caseid not in list(streaming_db.keys()):\n",
    "                case_bin.set_prefix_length(1)    \n",
    "                streaming_db[caseid] = []\n",
    "            elif caseid in finished_caseid:\n",
    "                pass\n",
    "            else:\n",
    "                case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "            # Encode event and cases and add to DB\n",
    "            case_bin.update_truelabel(outcome)   \n",
    "            case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "            ts = case_bin.event['ts']\n",
    "            streaming_db[caseid].append(case_bin)\n",
    "            # Detect label appeared case \n",
    "            if outcome != '' and caseid not in finished_caseid:\n",
    "                usedingrace.add(caseid)\n",
    "                for i in streaming_db[caseid]:\n",
    "                    i.update_truelabel(outcome)\n",
    "                finished_caseid.add(caseid)\n",
    "                # Adding newly finished case to training set.    \n",
    "                casecount +=1\n",
    "                # Grace period to collect feature matrix\n",
    "                if casecount < gp:\n",
    "                    case_length = len(streaming_db[caseid])\n",
    "                    if case_length >= maximum_prefix:\n",
    "                        if 'prefix_%s'%(maximum_prefix) not in list(feature_matrix.keys()):\n",
    "                            feature_matrix['prefix_%s'%(maximum_prefix)]=set()\n",
    "                            training_models['prefix_%s'%(maximum_prefix)] = [streaming_classifier,\n",
    "                                                                       0]\n",
    "                        feature_list = list(streaming_db[caseid][maximum_prefix-1].encoded.keys())\n",
    "                        for x in feature_list: feature_matrix['prefix_%s'%(maximum_prefix)].add(x) \n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if casecount not in train_window_dict.keys(): train_window_dict[casecount] = []\n",
    "        if casecount not in test_window_dict.keys(): test_window_dict[casecount] = []\n",
    "\n",
    "        for caseid in list(usedingrace):\n",
    "            case_length = len(streaming_db[caseid])\n",
    "            if case_length >= maximum_prefix:\n",
    "                x = streaming_db[caseid][maximum_prefix-1]\n",
    "                if x.prefix_length != 0:            \n",
    "                    training_windows.update_window(x)\n",
    "\n",
    "                update_test_cases(streaming_db[caseid], test_cases)\n",
    "        train_window_dict[casecount].append(copy.deepcopy(training_windows.container))\n",
    "\n",
    "        training_x = []\n",
    "        training_y = []\n",
    "        for pos, i in enumerate(training_windows.container):\n",
    "            x_prefix_length = i.prefix_length \n",
    "            i.encoded = utils.readjustment_training(i.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "            training_x.append(i.encoded)\n",
    "            training_y.append(i.true_label)\n",
    "\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        training_y = le.fit_transform(training_y)\n",
    "        training_x = pd.DataFrame.from_dict(training_x)\n",
    "\n",
    "        ###\n",
    "        #Oversampling\n",
    "        ###\n",
    "        n_labels = Counter(training_y)['True']\n",
    "\n",
    "        if n_labels <=2:\n",
    "            pass\n",
    "        elif n_labels>2 and n_labels <=5:\n",
    "            smote = SMOTE(k_neighbors=n_labels-1)\n",
    "            training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "        else:\n",
    "            smote = SMOTE()\n",
    "            training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "\n",
    "        start_time = time.time()\n",
    "        training_models['prefix_%s'%(x_prefix_length)][0].fit(training_x, training_y)\n",
    "        training_models['prefix_%s'%(x_prefix_length)][1] =casecount\n",
    "        end_time = time.time()\n",
    "        training_time.append(end_time-start_time)\n",
    "        \n",
    "        prediction_result[maximum_prefix][casecount] = {}\n",
    "        y_truelist = []\n",
    "        y_predlist = []\n",
    "        for case in test_cases:\n",
    "            if len(case) >= maximum_prefix:\n",
    "                x = case[maximum_prefix-1]\n",
    "                if x.prefix_length != 0:            \n",
    "                    model = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "                    current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                    current_event = pd.Series(current_event).to_frame().T\n",
    "                    y_pred = training_models['prefix_%s'%(x_prefix_length)][0].predict_proba(current_event)\n",
    "                    y_truelist.append(x.true_label)\n",
    "                    y_predlist.append(y_pred)\n",
    "                    test_window_dict[casecount].append(x)\n",
    "        prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "        prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "\n",
    "        train_variant = variant_list(train_window_dict[200][0])\n",
    "        save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "        '''\n",
    "        Streaming event label prediction start.\n",
    "        - Test and training steps are executed when case finished/ event arrived with label\n",
    "        '''\n",
    "        for i in streaming_db.keys(): usedingrace.add(i)\n",
    "        streaming_db ={}\n",
    "        cdhappend ={}\n",
    "        for i in range(1, maximum_prefix+1): cdhappend[i] = 0\n",
    "\n",
    "        for x,y in dataset:\n",
    "            display_progress(rowcounter, totallength)\n",
    "\n",
    "            rowcounter +=1\n",
    "            # Event stream change dictionary keys\n",
    "            x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "            if dataset_label !='bpic15':\n",
    "                x['ts'] = x['ts'][:-4]\n",
    "\n",
    "            # Check label possible\n",
    "            # x = utils.set_label(x)\n",
    "            x['outcome'] =y \n",
    "            # Initialize case by prefix length\n",
    "            caseid = x['caseid']\n",
    "            outcome = x['outcome']\n",
    "            x.pop('caseid')\n",
    "            x.pop('outcome')\n",
    "\n",
    "            if caseid not in usedingrace:\n",
    "                case_bin = prefix_bin(caseid, x)\n",
    "                case_bin.set_enctype(enctype)\n",
    "\n",
    "                if caseid not in list(streaming_db.keys()):\n",
    "                    case_bin.set_prefix_length(1)    \n",
    "                    streaming_db[caseid] = []\n",
    "                    running_case +=1\n",
    "                elif caseid in finished_caseid:\n",
    "                    pass\n",
    "                else:\n",
    "                    case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                    case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "                # Encode event and cases and add to DB\n",
    "                case_bin.update_truelabel(outcome)   \n",
    "                case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "                ts = case_bin.event['ts']\n",
    "\n",
    "                if case_bin.prefix_length == maximum_prefix:\n",
    "                    case_bin.encoded = utils.readjustment_training(case_bin.encoded, feature_matrix['prefix_%s'%(case_bin.prefix_length)])\n",
    "                streaming_db[caseid].append(case_bin)\n",
    "\n",
    "                # Detect label appeared case \n",
    "                if outcome != '' and caseid not in finished_caseid:\n",
    "                    finished_caseid.add(caseid)\n",
    "\n",
    "                    # Adding newly finished case to training set.\n",
    "                    casecount +=1    \n",
    "\n",
    "                    # Modify encoded attributes of cases with feature matrix\n",
    "                    case_length = len(streaming_db[caseid])\n",
    "                    if case_length >= maximum_prefix:\n",
    "\n",
    "                        streaming_db[caseid][maximum_prefix-1].update_truelabel(outcome)\n",
    "                        update_test_cases(streaming_db[caseid], test_cases)\n",
    "                        x = streaming_db[caseid][maximum_prefix-1].encoded\n",
    "                        prefix_length =streaming_db[caseid][maximum_prefix-1].prefix_length                    \n",
    "\n",
    "                        test_variant = variant_list([i[maximum_prefix-1] for i in test_cases])\n",
    "                        variant_cover = variant_coverage(train_variant, test_variant)/test_size\n",
    "                        label_dist = label_distribution(test_cases)\n",
    "                        training_windows.update_window(streaming_db[caseid][maximum_prefix-1])\n",
    "                        if retraining_condition == 'prodrift' or retraining_condition == 'prefixtreeCDD':\n",
    "                            if len(cd_list) ==0:\n",
    "                                retraining_trigger = False\n",
    "                                pass\n",
    "                            else:\n",
    "                                if test_cases[0][0].caseid == cd_list[0]:\n",
    "                                    retraining_trigger = True\n",
    "                                    cd_list.popleft()\n",
    "                                    print('Triggered1')\n",
    "                                else:\n",
    "                                    retraining_trigger = False\n",
    "                        \n",
    "                        if retraining_trigger == True:\n",
    "                            print('Triggered2')\n",
    "                            if casecount not in train_window_dict.keys(): train_window_dict[casecount] = []\n",
    "                            train_window_dict[casecount].append(copy.deepcopy(training_windows.container))                       \n",
    "\n",
    "                            x_training = pd.DataFrame.from_dict([i.encoded for i in training_windows.container])\n",
    "                            for i in x_training.columns.values: x_training[i] = x_training[i].fillna(0)\n",
    "                            feature_matrix['prefix_%s'%(maximum_prefix)] = x_training.columns.values\n",
    "\n",
    "                            training_x = []\n",
    "                            training_y = []\n",
    "                            for pos, i in enumerate(training_windows.container):\n",
    "                                x_prefix_length = i.prefix_length \n",
    "                                i.encoded = utils.readjustment_training(i.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                training_x.append(i.encoded)\n",
    "                                training_y.append(i.true_label)\n",
    "                            training_y = le.fit_transform(training_y)\n",
    "                            training_x = pd.DataFrame.from_dict(training_x)\n",
    "\n",
    "                            ###\n",
    "                            #Oversampling\n",
    "                            ###\n",
    "                            n_labels = Counter(training_y)['True']\n",
    "\n",
    "                            if n_labels <=2:\n",
    "                                pass\n",
    "                            elif n_labels>2 and n_labels <=5:\n",
    "                                smote = SMOTE(k_neighbors=n_labels-1)\n",
    "                                training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "                            else:\n",
    "                                smote = SMOTE()\n",
    "                                training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "                                                   ###\n",
    "                            #Model retraining\n",
    "                            ###\n",
    "                            start_time = time.time()\n",
    "                            training_models['prefix_%s'%(maximum_prefix)][0] = streaming_classifier\n",
    "                            training_models['prefix_%s'%(maximum_prefix)][0].fit(training_x, training_y)\n",
    "                            training_models['prefix_%s'%(x_prefix_length)][1] =casecount\n",
    "                            train_variant = variant_list(train_window_dict[casecount][0])\n",
    "                            end_time = time.time()\n",
    "                            training_time.append(end_time-start_time)\n",
    "                            \n",
    "                            if retraining_condition == 'label':\n",
    "                                train_window_dict = dict()\n",
    "\n",
    "                            save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = training_models['prefix_%s'%(x_prefix_length)]\n",
    "#                             save_model[training_models['prefix_%s'%(x_prefix_length)][1]] = 0\n",
    "                    y_truelist = []\n",
    "                    y_predlist = []\n",
    "\n",
    "                    if casecount not in test_window_dict.keys():\n",
    "                        test_window_dict[casecount] = []\n",
    "\n",
    "                    for case in test_cases:\n",
    "                        if len(case) >= maximum_prefix:\n",
    "                            x = case[maximum_prefix-1]\n",
    "                            if x.prefix_length != 0:\n",
    "                                length = x.prefix_length\n",
    "                                current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                                current_event = pd.Series(current_event).to_frame().T\n",
    "\n",
    "                                if casecount != gp:\n",
    "                                    c_id = [x.caseid for x in test_window_dict[casecount-1]]\n",
    "\n",
    "                                if x.caseid not in c_id:\n",
    "                                    y_pred = training_models['prefix_%s'%(x_prefix_length)][0].predict_proba(current_event)\n",
    "                                else:\n",
    "                                    y_pred = prediction_result[maximum_prefix][casecount-1]['y_pred'][c_id.index(x.caseid)]\n",
    "                                y_truelist.append(x.true_label)\n",
    "                                y_predlist.append(y_pred)\n",
    "\n",
    "                                if casecount not in test_window_dict.keys(): test_window_dict[casecount] = []\n",
    "                                test_window_dict[casecount].append(x)\n",
    "                        prediction_result[maximum_prefix][casecount] = {}\n",
    "                        prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "                        prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "    #                 if 'b1' not in caseid and cdhappend[maximum_prefix] == 0:\n",
    "    #                     cdhappend[maximum_prefix] = model_update_count\n",
    "\n",
    "        try:\n",
    "            os.makedirs('../result/%s/%s/Finished cases/Trigger %s'%(dataset_label, classifier, retraining_condition))\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s training window retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(train_window_dict, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s test window retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(test_window_dict, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s update retrained.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(prediction_result, f)\n",
    "        with gzip.open('../result/%s/%s/Finished cases/Trigger %s/prefix_%s model.pkl'%(dataset_label, classifier, retraining_condition, maximum_prefix), 'wb') as f:\n",
    "            pkl.dump(save_model, f)\n",
    "        with gzip.open('../result/time/%s_%s_%s_%s_trainingtime.pkl'%(dataset_label, classifier, maximum_prefix, retraining_condition), 'wb') as f:\n",
    "            pkl.dump(training_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb065bf7-d585-4ed6-91b9-ec69df04dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_cd(dataset_label, cd_type):\n",
    "    file_path = '../triggered_cd/%s/%s_CD_list.pkl'%(cd_type, dataset_label)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "199824a1-bf03-488a-833e-6091e011926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigger_cd('bpic2015_3','prodrift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea8df4-ee62-49ec-8a7c-9ec6e66a9408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
