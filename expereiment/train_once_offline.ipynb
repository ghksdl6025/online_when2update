{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2839db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from river import stream,tree,metrics\n",
    "from encoding import prefix_bin\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import os,json\n",
    "import datetime\n",
    "from collections import deque, Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import xgboost as xgb\n",
    "\n",
    "import datetime, time\n",
    "import utils\n",
    "import sliding_window\n",
    "import psutil\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c57d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(row_counting, total_length, interval=2000):\n",
    "    if rowcounter%interval == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running cases: %s'%(len(streaming_db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcbf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_cases(new_case, test_cases):\n",
    "    test_cases.append(new_case)\n",
    "    if len(test_cases) > test_size:\n",
    "        test_cases.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e157bbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_pair': {'Case ID': 'caseid', 'Activity': 'activity', 'Complete Timestamp': 'ts'}, 'categorical_attrs': ['activity'], 'maximum_prefix': 12}\n",
      "1\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "2\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "3\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "4\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "5\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "6\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "7\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "8\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "9\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "10\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "11\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n",
      "12\n",
      "37.19 % Case finished: 375 Running cases: 181\n",
      "55.78 % Case finished: 562 Running cases: 368\n",
      "74.38 % Case finished: 739 Running cases: 548\n",
      "92.97 % Case finished: 917 Running cases: 727\n"
     ]
    }
   ],
   "source": [
    "# Experiment settings\n",
    "'./DATA/logs/synthetic_log_bc1.csv'\n",
    "dataset_label = 'IRO5K'\n",
    "\n",
    "# Invoke parameters for dataset\n",
    "window_size = 100\n",
    "test_size = 30\n",
    "gp = 200\n",
    "training_rebalance = False\n",
    "with open('../dataset_parameters.json','r') as json_file:\n",
    "    parameters = json.load(json_file)[dataset_label]\n",
    "    print(parameters)\n",
    "    key_pair = parameters['key_pair']\n",
    "    catatars = parameters['categorical_attrs']\n",
    "    maximum_prefixs = parameters['maximum_prefix']\n",
    "\n",
    "\n",
    "for maximum_prefix in range(1, maximum_prefixs+1):\n",
    "    print(maximum_prefix)\n",
    "    train_window_dict = {}\n",
    "    test_window_dict = {}\n",
    "    dataset_loc = '../DATA/logs/'+ dataset_label +'.csv'\n",
    "    try:\n",
    "        os.makedirs('../result/%s'%(dataset_label))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Set streaming classifier\n",
    "    classifier = 'rf'\n",
    "    if classifier == 'htc':\n",
    "        streaming_classifier = tree.HoeffdingTreeClassifier(grace_period=10, split_criterion='info_gain')\n",
    "    elif classifier =='hatc':\n",
    "        streaming_classifier = tree.HoeffdingAdaptiveTreeClassifier(grace_period=10, split_criterion='info_gain' )\n",
    "    elif classifier == 'efdt':\n",
    "        streaming_classifier = tree.ExtremelyFastDecisionTreeClassifier(grace_period=10, split_criterion='info_gain')\n",
    "    elif classifier == 'rf':\n",
    "        streaming_classifier = RandomForestClassifier(n_estimators = 10, n_jobs = -1)\n",
    "    elif classifier == 'xgb':\n",
    "        streaming_classifier = xgb.XGBClassifier(n_estimators = 10, learning_rate=0.01, verbosity =0)     \n",
    "    dataset = stream.iter_csv(\n",
    "            dataset_loc\n",
    "            )\n",
    "\n",
    "    totallength = len(list(dataset))\n",
    "\n",
    "    dataset = stream.iter_csv(\n",
    "            dataset_loc,\n",
    "            # drop=['Complete Timestamp'],\n",
    "            target='outcome'\n",
    "            )\n",
    "    enctype = 'Index-base'\n",
    "    \n",
    "    streaming_db ={}\n",
    "    training_windows = sliding_window.training_window(window_size,test_size)\n",
    "    training_models ={}\n",
    "    test_cases = deque()\n",
    "    feature_matrix ={}\n",
    "\n",
    "    save_model = {}\n",
    "    casecount = 0\n",
    "    rowcounter = 0\n",
    "    finished_db ={}\n",
    "    running_case = 0\n",
    "    window_acc_dict = {}\n",
    "    prediction_result = dict()\n",
    "    for i in range(1, maximum_prefix+1): prediction_result[i] = {}\n",
    "    finished_caseid = set()\n",
    "    usedingrace = set()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Before test and training streaming event predictive monitoring, grace period is preceded to initialize model by prefix length\n",
    "    and obtain feature matrix to transform future events\n",
    "    '''\n",
    "    for x,y in dataset:\n",
    "#         display_progress(rowcounter, totallength)\n",
    "        rowcounter +=1\n",
    "        # Event stream change dictionary keys\n",
    "        x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "        if dataset_label !='bpic15':\n",
    "            x['ts'] = x['ts'][:-4]\n",
    "\n",
    "        x['outcome'] =y \n",
    "        # Initialize case by prefix length        \n",
    "        caseid = x['caseid']\n",
    "        outcome = x['outcome']\n",
    "    #     progress = x['progress']\n",
    "\n",
    "        x.pop('caseid')\n",
    "        x.pop('outcome')\n",
    "\n",
    "    #     x.pop('progress')\n",
    "\n",
    "        case_bin = prefix_bin(caseid, x)\n",
    "        case_bin.set_enctype(enctype)\n",
    "\n",
    "        if caseid not in list(streaming_db.keys()):\n",
    "            case_bin.set_prefix_length(1)    \n",
    "            streaming_db[caseid] = []\n",
    "        elif caseid in finished_caseid:\n",
    "            pass\n",
    "        else:\n",
    "            case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "            case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "        # Encode event and cases and add to DB\n",
    "        case_bin.update_truelabel(outcome)   \n",
    "        case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "        ts = case_bin.event['ts']\n",
    "        streaming_db[caseid].append(case_bin)\n",
    "        # Detect label appeared case \n",
    "        if outcome != '' and caseid not in finished_caseid:\n",
    "            usedingrace.add(caseid)\n",
    "            for i in streaming_db[caseid]:\n",
    "                i.update_truelabel(outcome)\n",
    "            finished_caseid.add(caseid)\n",
    "            # Adding newly finished case to training set.    \n",
    "            casecount +=1\n",
    "            # Grace period to collect feature matrix\n",
    "            if casecount < gp:\n",
    "                case_length = len(streaming_db[caseid])\n",
    "                if case_length >= maximum_prefix:\n",
    "                    if 'prefix_%s'%(maximum_prefix) not in list(feature_matrix.keys()):\n",
    "                        feature_matrix['prefix_%s'%(maximum_prefix)]=set()\n",
    "                        training_models['prefix_%s'%(maximum_prefix)] = [streaming_classifier,\n",
    "                                                                   0]\n",
    "                    feature_list = list(streaming_db[caseid][maximum_prefix-1].encoded.keys())\n",
    "                    for x in feature_list: feature_matrix['prefix_%s'%(maximum_prefix)].add(x) \n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    if casecount not in train_window_dict.keys(): train_window_dict[casecount] = []\n",
    "    if casecount not in test_window_dict.keys(): test_window_dict[casecount] = []\n",
    "\n",
    "    for caseid in list(usedingrace):\n",
    "        case_length = len(streaming_db[caseid])\n",
    "        if case_length >= maximum_prefix:\n",
    "            x = streaming_db[caseid][maximum_prefix-1]\n",
    "            if x.prefix_length != 0:            \n",
    "                training_windows.update_window(x)\n",
    "    \n",
    "            update_test_cases(streaming_db[caseid], test_cases)\n",
    "    train_window_dict[casecount].append(copy.deepcopy(training_windows.container))\n",
    "    \n",
    "    x_prefix_length = 0\n",
    "    training_x = []\n",
    "    training_y = []\n",
    "    for pos, i in enumerate(training_windows.container):\n",
    "        x_prefix_length = i.prefix_length \n",
    "        i.encoded = utils.readjustment_training(i.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "        training_x.append(i.encoded)\n",
    "        training_y.append(i.true_label)\n",
    "\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    training_y = le.fit_transform(training_y)\n",
    "    training_x = pd.DataFrame.from_dict(training_x)\n",
    "    \n",
    "    ###\n",
    "    #Oversampling\n",
    "    ###\n",
    "    if training_rebalance == True:\n",
    "        smote = SMOTE()\n",
    "        training_x, training_y = smote.fit_resample(training_x, training_y)\n",
    "        \n",
    "    training_models['prefix_%s'%(x_prefix_length)][0].fit(training_x, training_y)\n",
    "    training_models['prefix_%s'%(x_prefix_length)][1] =casecount\n",
    "    \n",
    "    prediction_result[maximum_prefix][casecount] = {}\n",
    "    y_truelist = []\n",
    "    y_predlist = []\n",
    "    for case in test_cases:\n",
    "        if len(case) >= maximum_prefix:\n",
    "            x = case[maximum_prefix-1]\n",
    "            if x.prefix_length != 0:            \n",
    "                model = training_models['prefix_%s'%(x_prefix_length)][0]\n",
    "                current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                current_event = pd.Series(current_event).to_frame().T\n",
    "                y_pred = training_models['prefix_%s'%(x_prefix_length)][0].predict_proba(current_event)\n",
    "                y_truelist.append(x.true_label)\n",
    "                y_predlist.append(y_pred)\n",
    "                test_window_dict[casecount].append(x)\n",
    "    prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "    prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Streaming event label prediction start.\n",
    "    - Test and training steps are executed when case finished/ event arrived with label\n",
    "    '''\n",
    "    for i in streaming_db.keys(): usedingrace.add(i)\n",
    "    streaming_db ={}\n",
    "    cdhappend ={}\n",
    "    for i in range(1, maximum_prefix+1): cdhappend[i] = 0\n",
    "    for x,y in dataset:\n",
    "        display_progress(rowcounter, totallength)\n",
    "\n",
    "        rowcounter +=1\n",
    "        # Event stream change dictionary keys\n",
    "        x = utils.dictkey_chg(x, key_pair)\n",
    "\n",
    "        if dataset_label !='bpic15':\n",
    "            x['ts'] = x['ts'][:-4]\n",
    "\n",
    "        # Check label possible\n",
    "        # x = utils.set_label(x)\n",
    "        x['outcome'] =y \n",
    "        # Initialize case by prefix length\n",
    "        caseid = x['caseid']\n",
    "        outcome = x['outcome']\n",
    "        x.pop('caseid')\n",
    "        x.pop('outcome')\n",
    "\n",
    "        if caseid not in usedingrace:\n",
    "            case_bin = prefix_bin(caseid, x)\n",
    "            case_bin.set_enctype(enctype)\n",
    "\n",
    "            if caseid not in list(streaming_db.keys()):\n",
    "                case_bin.set_prefix_length(1)    \n",
    "                streaming_db[caseid] = []\n",
    "                running_case +=1\n",
    "            elif caseid in finished_caseid:\n",
    "                pass\n",
    "            else:\n",
    "                case_bin.set_prefix_length(len(streaming_db[caseid])+1)\n",
    "                case_bin.set_prev_enc(streaming_db[caseid][-1])\n",
    "\n",
    "            # Encode event and cases and add to DB\n",
    "            case_bin.update_truelabel(outcome)   \n",
    "            case_bin.update_encoded(catattrs=catatars,enctype=enctype)\n",
    "            ts = case_bin.event['ts']\n",
    "\n",
    "            if case_bin.prefix_length == maximum_prefix:\n",
    "                case_bin.encoded = utils.readjustment_training(case_bin.encoded, feature_matrix['prefix_%s'%(case_bin.prefix_length)])\n",
    "            streaming_db[caseid].append(case_bin)\n",
    "\n",
    "            # Detect label appeared case \n",
    "            if outcome != '' and caseid not in finished_caseid:\n",
    "                finished_caseid.add(caseid)\n",
    "\n",
    "                # Adding newly finished case to training set.\n",
    "                casecount +=1    \n",
    "\n",
    "                # Modify encoded attributes of cases with feature matrix\n",
    "                case_length = len(streaming_db[caseid])\n",
    "                if case_length >= maximum_prefix:\n",
    "\n",
    "                    update_test_cases(streaming_db[caseid], test_cases)\n",
    "                    streaming_db[caseid][maximum_prefix-1].update_truelabel(outcome)\n",
    "                    x = streaming_db[caseid][maximum_prefix-1].encoded\n",
    "                    prefix_length =streaming_db[caseid][maximum_prefix-1].prefix_length                    \n",
    "#                     save_model['%s_%s'%(prefix_length, model_update_count)] = training_models['prefix_%s'%(streaming_db[caseid][maximum_prefix-1].prefix_length)][0]\n",
    "\n",
    "                y_truelist = []\n",
    "                y_predlist = []\n",
    "\n",
    "                if casecount not in test_window_dict.keys():\n",
    "                    test_window_dict[casecount] = []\n",
    "                    \n",
    "                for case in test_cases:\n",
    "                    if len(case) >= maximum_prefix:\n",
    "                        x = case[maximum_prefix-1]\n",
    "                        if x.prefix_length != 0:\n",
    "                            length = x.prefix_length\n",
    "                            current_event = utils.readjustment_training(x.encoded, feature_matrix['prefix_%s'%(maximum_prefix)])\n",
    "                            current_event = pd.Series(current_event).to_frame().T\n",
    "                            \n",
    "                            if casecount != gp:\n",
    "                                c_id = [x.caseid for x in test_window_dict[casecount-1]]\n",
    "                                \n",
    "                            if x.caseid not in c_id:\n",
    "                                y_pred = training_models['prefix_%s'%(x_prefix_length)][0].predict_proba(current_event)\n",
    "                            else:\n",
    "                                y_pred = prediction_result[maximum_prefix][casecount-1]['y_pred'][c_id.index(x.caseid)]\n",
    "                            y_truelist.append(x.true_label)\n",
    "                            y_predlist.append(y_pred)\n",
    "                            \n",
    "                            if casecount not in test_window_dict.keys():\n",
    "                                test_window_dict[casecount] = []\n",
    "                            test_window_dict[casecount].append(x)\n",
    "                prediction_result[maximum_prefix][casecount] = {}\n",
    "                prediction_result[maximum_prefix][casecount]['y_true'] = y_truelist\n",
    "                prediction_result[maximum_prefix][casecount]['y_pred'] = y_predlist\n",
    "#                 if 'b1' not in caseid and cdhappend[maximum_prefix] == 0:\n",
    "#                     cdhappend[maximum_prefix] = model_update_count\n",
    "    \n",
    "    try:\n",
    "        os.makedirs('../result/%s/%s/Finished cases/'%(dataset_label, classifier))\n",
    "    except:\n",
    "        pass    \n",
    "    \n",
    "    with open('../result/%s/%s/Finished cases/train_rebalance_%s prefix_%s training window.pkl'%(dataset_label, classifier, str(training_rebalance), maximum_prefix), 'wb') as f:\n",
    "        pkl.dump(train_window_dict, f)\n",
    "    with open('../result/%s/%s/Finished cases/train_rebalance_%s prefix_%s test window.pkl'%(dataset_label, classifier, str(training_rebalance), maximum_prefix), 'wb') as f:\n",
    "        pkl.dump(test_window_dict, f)\n",
    "\n",
    "    with open('../result/%s/%s/Finished cases/train_rebalance_%s prefix_%s update.pkl'%(dataset_label, classifier, str(training_rebalance), maximum_prefix), 'wb') as f:\n",
    "        pkl.dump(prediction_result, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "615cd0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../result/IRO5K/xgb/Finished cases/train_rebalance_True prefix_12 update.pkl'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'../result/%s/%s/Finished cases/train_rebalance_%s prefix_%s update.pkl'%(dataset_label, classifier, str(training_rebalance), maximum_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
